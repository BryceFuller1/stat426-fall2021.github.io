<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-11-08T16:24:40-08:00</updated><id>/feed.xml</id><title type="html">Stat 426 - Fall 2021</title><subtitle>Class Blog and Projects</subtitle><entry><title type="html">Cancelling Noise in Data: Image Compression Tutorial</title><link href="/blog/svd-img-compression" rel="alternate" type="text/html" title="Cancelling Noise in Data: Image Compression Tutorial" /><published>2021-11-06T00:00:00-07:00</published><updated>2021-11-06T00:00:00-07:00</updated><id>/blog/svd-img-compression</id><content type="html" xml:base="/blog/svd-img-compression">&lt;p&gt;One common problem  in data science is that our data is noisy. A good data scientist
is able to distinguish between that noise and signal in the data. One great way
to “cancel noise” in data represented in matrix form is to use singular value decomposition
from linear algebra.&lt;/p&gt;

&lt;h1 id=&quot;linear-algebra&quot;&gt;Linear Algebra&lt;/h1&gt;
&lt;p&gt;Let me explain some key concepts first.&lt;/p&gt;
&lt;div class=&quot;tenor-gif-embed&quot; data-postid=&quot;8939664&quot; data-share-method=&quot;host&quot; data-aspect-ratio=&quot;1.86047&quot; data-width=&quot;100%&quot;&gt;&lt;a href=&quot;https://tenor.com/view/let-me-explain-sum-up-too-much-i-just-cant-princess-bride-gif-8939664&quot;&gt;Let Me Explain Sum Up GIF&lt;/a&gt;from &lt;a href=&quot;https://tenor.com/search/let+me+explain-gifs&quot;&gt;Let Me Explain GIFs&lt;/a&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://tenor.com/embed.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Singular value decomposition (SVD) is a method that breaks a matrix into three less-complicated
matrices. One convenient feature of these three matrices is that the values inside them are
sorted such that the most important attributes come at the beginning– that makes it easy to
extract the features that hold the most predictive power. If you want to take a deeper dive into
linear algebra and how it will help you be a better data scientist, check out this excellent blog post from
&lt;a href=&quot;https://towardsdatascience.com/understanding-singular-value-decomposition-and-its-application-in-data-science-388a54be95d&quot;&gt;Toward Data Science&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;With that in mind, it’s no wonder that Benjamin Obi Tayo, Ph.D. wrote on &lt;a href=&quot;https://www.kdnuggets.com/2021/05/essential-linear-algebra-data-science-machine-learning.html&quot;&gt;KDnuggets&lt;/a&gt;:
“Linear algebra is the most important math skill in machine learning.” Let’s take a look at an example of how to use SVD to reduce image sizes!&lt;/p&gt;

&lt;h1 id=&quot;tutorial&quot;&gt;Tutorial&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;Open up a new notebook your favorite python editor. I like to use Jupyter notebooks, but Google Colab is also a great option if you
don’t have python downloaded on your computer.&lt;/li&gt;
  &lt;li&gt;Download a PNG image to play with. I’ll use this one:&lt;img src=&quot;/assets/images/blogimages/figs-11-06/oscar-sutton-yihlaRCCvd4-unsplash.png&quot; alt=&quot;dog&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;If you’re using Jupyter, add the photo to the same directory as your notebook. If you’re 
using Colab, go to the left sidebar, and click the “Upload to session storage” button. You’ll have to do this each time you
close the notebook and come back to it later.
&lt;img src=&quot;/assets/images/blogimages/figs-11-06/colab_upload_img.PNG&quot; alt=&quot;upload-to_colab&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;In the first cell, add these imports:
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;skimage&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;skimage&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Now we need to read in our image file as a python array. You can choose whether to keep it as a color (RGB) image
or convert it to grayscale.
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;color_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dog.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;            
&lt;span class=&quot;n&quot;&gt;gray_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;skimage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rgb2gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Now that we have our images, let’s define some functions that will plot an array as an image to help us see what we have.
```python
def show_color(array):
 # set up the matplotlib figure
 plt.figure(figsize=(10,10))
 # optional-add grid lines to figure
 plt.grid(None)
 # show the array as an image within the figure
 plt.imshow(array)
 return None&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;def show_gray(array):
    # set up the matplotlib figure
    plt.figure(figsize=(10,10))
    # optional-add grid lines to figure
    plt.grid(None)
    # show the array, but specify that the color map is gray with a min val of 0 (white) and max of 1 (black)
    plt.imshow(array,cmap=’gray’,vmin=0,vmax=1)
    return None&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;7. Test your functions by printing out either or both the color and grayscale images. For the rest of the tutorial,
I will use the grayscale image.
```python
show_color(color_array)
show_gray(gray_array)  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ol&gt;
  &lt;li&gt;Now let’s calculate the number of values in this matrix by multiplying the number of rows by the number of columns.
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gray_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;original_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;original_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Now we can call the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;svd&lt;/code&gt; function to decompose &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gray_array&lt;/code&gt; into three matrices. Typically, these three matrices are
called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;U&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sigma&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V^t&lt;/code&gt; so we will name them accordingly. If we wanted to, we could use these to reconstruct an
approximation of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gray_array&lt;/code&gt;.
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;u_orig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s_orig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_t_orig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gray_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;In reality, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s_orig&lt;/code&gt; is actually a 1-dimensional array instead of a matrix (2-dimensional array). This is because
the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sigma&lt;/code&gt; matrix has 0’s everywhere except the diagonal from the top left corner, so we only need one dimension to
contain all the nonzero values, which are called “singular values.” These are arranged from largest to smallest.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Graph the singular values in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s_orig&lt;/code&gt; against their index in the array. This will let us get a good idea how many
singular values we will need to make a good approximation of our image without losing too much quality.
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s_orig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-11-06/plot.PNG&quot; alt=&quot;plot&quot; /&gt;
Since the line drops to nearly 0 when x (index) is still very small, this means that we will be able to make a good
approximation using just a few of the singular values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;In order to reconstruct our reduced-size image matrix, we’re going to need to define a few functions.
The first is a function that takes in two ints that represent the number of rows and columns as well as an array of
singular values, and returns a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sigma&lt;/code&gt; matrix.
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;E&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;E&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;That creates a matrix of the correct size filled with 0’s, and then goes along the diagonal and changes the 0 to the
corresponding singular value from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The second function takes in a 2D array &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;u&lt;/code&gt;, a 1D array &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt;, and a 2D array &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v_t&lt;/code&gt;, which correspond to the 3 outputs
from calling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;svd&lt;/code&gt;. It multiplies them together to reconstruct a single matrix.
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;reconstructed_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;sgm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;usgm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sgm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;usgmvt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usgm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;usgmvt&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;The third function brings everything together to produce a rank &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; approximation of a matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt;. That essentially
translates to keeping a matrix with only the top &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; singular values.
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lower_rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;u_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;v_t_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:]&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;s_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reconstructed_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_t_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Now you can plug in different values of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; into your function along with our original &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gray_array&lt;/code&gt; and you can
decide what value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; you think is best. The smaller the value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt;, the more compressed our final image will be,
but be careful to not make it too fuzzy. The best value will vary for different pictures.
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choose&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gray_reduced&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lower_rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gray_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;show_gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gray_reduced&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;As it is right now, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gray_reduced&lt;/code&gt; has the exact same dimensions as the original &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gray_array&lt;/code&gt; matrix. However, now
we can call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;svd&lt;/code&gt; on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gray_reduced&lt;/code&gt; and we can get rid of anything we don’t need. In reality, we only need the first &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt;
only need the first &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; columns of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;u&lt;/code&gt;, the first &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; rows of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v_t&lt;/code&gt;, and the first &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; values of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sigma&lt;/code&gt;. That will tell
us how many values we really need to represent the image. We can use that to find the number of values we need.
```python
u_reduced,s_reduced,v_t_reduced = np.linalg.svd(gray_reduced)
reduced_size = ((len(u_reduced[0]) * k) + (len(v_t_reduced) * k) + k)
print(“Reduced size: “, reduced_size)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;relative_size= reduced_size/original_size
print(“Original size: “, original_size)
print(“Reduced size is what percent of original size: “, relative_size)
```&lt;/p&gt;

&lt;p&gt;Congratulations! You have now compressed an image using singular value decomposition!&lt;/p&gt;

&lt;h2 id=&quot;attributions&quot;&gt;Attributions&lt;/h2&gt;

&lt;p&gt;Cover photo by &lt;a href=&quot;https://unsplash.com/@millarjb?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot;&gt;Jack Millard&lt;/a&gt; on &lt;a href=&quot;https://unsplash.com/s/photos/magnifying-glass?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot;&gt;Unsplash&lt;/a&gt;
This post is adapted from a lab from the Computational Linear Algebra class at Brigham Young University.&lt;/p&gt;</content><author><name>Jessica Hamblin</name><email>hamblinjm@hotmail.com</email></author><category term="image compression" /><category term="reduction" /><category term="linear algebra" /><category term="svd" /><category term="noise" /><summary type="html">One common problem in data science is that our data is noisy. A good data scientist is able to distinguish between that noise and signal in the data. One great way to “cancel noise” in data represented in matrix form is to use singular value decomposition from linear algebra.</summary></entry><entry><title type="html">Let’s talk JSON Basics</title><link href="/blog/json" rel="alternate" type="text/html" title="Let’s talk JSON Basics" /><published>2021-11-04T00:00:00-07:00</published><updated>2021-11-04T00:00:00-07:00</updated><id>/blog/json</id><content type="html" xml:base="/blog/json">&lt;p&gt;The first time I heard the term JSON, I thought that it was someone emphasizing a name weird. I’m probably alone in this error, but hopefully this post can help someone avoid my blunder and avoid embarrasment when asking which Jason we were talking about. &lt;em&gt;[I really wish I could claim this was hyperbole.]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Well, you have to make mistakes to learn. I now know that JSON actually means: JavaScript Object Notation…&lt;/p&gt;

&lt;p&gt;JSON is a format for storing and transporting data, created to
be more readable for humans, as opposed to machine language or more complicated ways of data transport. Often JSON is referenced as being self-describing, and therefore easy to read. While inital experience with JSON might make you feel otherwise, experience and comparison to other methods quickly reveals JSON is quite nice. JSON itself is a more recent asset, credited to creators Douglas Crockford and Chip Morningstar in 2001. As the name denotes, JSON is derived from Javascript. However, luckily for all, JSON itself is language-independent, meaning that all programming languages can be adapted to parse JSON.&lt;/p&gt;

&lt;p&gt;JSON contains data composed of attribute-value pairs and arrays,
somewhat comparable to a Python dictionary. A key value is associated with an array of data, usually obtained by indexing.
JSON is recognizable as each data item is contained in curly braces. Inside the curly braces, a key value is followed by a colon,
with the values typically stored in brackets. JSON does not support comments, so any comments below exist for instruction purposes only. Also, some values have been arranged in differing ways to show how differing formats and spacing appears.
Here is an example to show JSON in action:&lt;/p&gt;
&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;first_array&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
	&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;stat 426&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
	&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;is an&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
	&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;amazing class&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;If you wanted to have more than one key based array, you would simply put a comma after the values of the previous array:&lt;/p&gt;
&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;first_array&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
	&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;stat 426&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
	&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;is an&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
	&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;amazing class&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;comma&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;here&lt;/span&gt;
 &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;second_array&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	 &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Inside an array, you can have stand alone values or you could have nested data. Again, this is best shown in an example:&lt;/p&gt;
&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;outside_array&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;inside_array&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;variable&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;hello!&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here we have the data inside brackets. First we have the &lt;em&gt;outside_array&lt;/em&gt;, which contains the standalone &lt;em&gt;variable&lt;/em&gt; and a nested array &lt;em&gt;inside_array&lt;/em&gt;.
Inside the &lt;em&gt;inside_array&lt;/em&gt;, we have key-value pairs.&lt;/p&gt;

&lt;p&gt;Hopefully that gives a general idea behind the structure and potential that JSON holds. Let’s jump over to see how you would access the values inside. Actually obtaining the JSON is dependent on how it is stored and which language or method used to retrieve. For brevity’s sake we will focus on the general idea of obtaining values in the Javascript language. This idea translates nicely into other programming languages.&lt;/p&gt;

&lt;p&gt;Looking at the last example, let’s call the whole item contained in the first pair of braces &lt;em&gt;Data&lt;/em&gt;.
&lt;em&gt;Data&lt;/em&gt; is an object containing a singular &lt;em&gt;outside_array&lt;/em&gt;, and to get inside we would index &lt;em&gt;Data&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To access values inside Data, we would use dot notation or bracket notation.
Examples to access the &lt;em&gt;outside_array&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nx&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;outside_array&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;oustide_array&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To access the inside_array, we will follow the same above methods:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nx&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;outside_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;inside_array&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;outside_array&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;inside_array&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;outside_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again, this is the idea behind accessing data in JSON. You can get JSON from varied sources, including APIs and URLs. Accessing each is also dependent on your environment and programming language. Most however do follow this general idea of indexing to find the desired value.
For Python help: https://docs.python.org/3/library/json.html&lt;/p&gt;

&lt;p&gt;Python JSON library&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;</content><author><name>Cal Barker</name><email>calbarker1@gmail.com</email></author><category term="JSON" /><category term="JavaScript" /><category term="Python" /><summary type="html">The first time I heard the term JSON, I thought that it was someone emphasizing a name weird. I’m probably alone in this error, but hopefully this post can help someone avoid my blunder and avoid embarrasment when asking which Jason we were talking about. [I really wish I could claim this was hyperbole.]</summary></entry><entry><title type="html">Hyperparameter Tuning Basics</title><link href="/blog/hyperparameter-tuning-basics" rel="alternate" type="text/html" title="Hyperparameter Tuning Basics" /><published>2021-11-02T00:00:00-07:00</published><updated>2021-11-02T00:00:00-07:00</updated><id>/blog/hyperparameter-tuning-basics</id><content type="html" xml:base="/blog/hyperparameter-tuning-basics">&lt;h1 id=&quot;a-quickstart-to-hyperparameter-tuning&quot;&gt;A Quickstart to Hyperparameter Tuning&lt;/h1&gt;

&lt;h2 id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Introduction to Hyperparameters&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Setting Up&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Grid Search&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Random Search&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Which Should I Use?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Extra: Bayesian Optimization&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this blog, I hope to show you some of the basics to get started with hyperparameter tuning. 
If you are a data scientist like me then you are probably wanting to use machine learning to solve all your problems. 
If that’s the case, then learning to tune your model will be pretty important.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.redd.it/nc5ua4x8lfg31.png&quot; alt=&quot;img_meme1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once you have hyperparameter tuning down, then you are one step closer to getting a generalizeable model set up for your different needs. Let’s dive into it!&lt;/p&gt;

&lt;h4 id=&quot;intro-to-hyperparameter-tuning&quot;&gt;Intro to Hyperparameter Tuning&lt;/h4&gt;

&lt;p&gt;A hyperparameter is a parameter that is set by the user before the machine learning process begins. They are different
from model parameters, which are tuned by a machine learning algorithm automatically.
Hyperparameters come in many different forms including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;K in KNN&lt;/li&gt;
  &lt;li&gt;Alpha in LASSO regression&lt;/li&gt;
  &lt;li&gt;The learning rate for a neural network&lt;/li&gt;
  &lt;li&gt;And the &lt;a href=&quot;https://xgboost.readthedocs.io/en/latest/parameter.html&quot;&gt;wide variety of hyperparameters for XGBoost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Essentially, the hyperparameters represent the “knobs,” “dials,” and “switches” that
are moved around—as we feed an algorithm data—to produce the optimal model for the dataset.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://imgs.xkcd.com/comics/machine_learning.png&quot; alt=&quot;img_meme2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With that in mind, it can be overwhelming to memorize all the different hyperparameters of each model and 
which values would help them function best on a dataset. Often, it is not clear how the hyperparameters
will affect performance on a dataset and many hyperparameters interact with each other in a non-linear fashion. So beware:
because trying to tune a model completely by hand can be an easy way to make yourself go crazy.&lt;/p&gt;

&lt;p&gt;Fortunately, data scientists have found a few algorithmic approaches to automating the process of 
hyperparameter tuning so that you don’t have to do it yourself. In this blog, we will discuss three different
methods for tuning hyperparameters: grid search, random search, and briefly on bayesian optimization.&lt;/p&gt;

&lt;h2 id=&quot;hyperparameter-tuning-with-scikit-learn&quot;&gt;Hyperparameter Tuning with Scikit-Learn&lt;/h2&gt;

&lt;h4 id=&quot;setting-up-the-model-and-search-space&quot;&gt;Setting up the Model and Search Space&lt;/h4&gt;

&lt;p&gt;We will use the Scikit-Learn API to set up our model and run our hyperparameter tuning.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# importing Logistic Regressiona and dataset
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_blobs&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Get blob data
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_blobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster_std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we want to set up the search space we will be using for our Logistic Regression hyperparameters.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Our chosen hyperparameters
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;c1&quot;&gt;# sample between different solvers
&lt;/span&gt;          &lt;span class=&quot;s&quot;&gt;'solver'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'newton-cg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'lbfgs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'liblinear'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
          &lt;span class=&quot;c1&quot;&gt;# Set our penalty as l2
&lt;/span&gt;          &lt;span class=&quot;s&quot;&gt;'penalty'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
          &lt;span class=&quot;c1&quot;&gt;# Chose between a range of our regularization values
&lt;/span&gt;          &lt;span class=&quot;s&quot;&gt;'C'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are more hyperparameters to choose from and they can be found and added from the &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html&quot;&gt;logistic regression documentation.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With all of our hyperparameters set, we can run it through our grid search method.&lt;/p&gt;

&lt;h4 id=&quot;grid-search&quot;&gt;Grid Search&lt;/h4&gt;

&lt;p&gt;The grid search method is very similar to random search when it comes to hyperparameter tuning. 
In a nutshell, grid search will build on every hyperparameter combination possible in the given search space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pros&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Exhaustive, it will find the best combination out of the given hyperparameter space&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cons&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Computationally expensive, the exhaustive search can become too computationally expensive if 
there are too many hyperparameter combinations to try out&lt;/li&gt;
  &lt;li&gt;Overfitting, there is the possibility of overfitting to a specific space the search&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s finish our code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GridSearchCV&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Set up our Grid search and ad our model to it
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GridSearchCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;estimator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param_grid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Find results
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Best Score: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_score_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Best Hyperparameters: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_params_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It may take some time to run the model, but after it finished it will report the best score and 
hyperparameter combinations. We can see it here:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Best&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9872799999999999&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Best&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Hyperparameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'penalty'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'l2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'solver'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'liblinear'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now you can go ahead and use those hyperparameters on the test set!&lt;/p&gt;

&lt;p&gt;Next, we will do the same with a random forest model, but this time we will
try out the random search method.&lt;/p&gt;

&lt;h4 id=&quot;random-search&quot;&gt;Random Search&lt;/h4&gt;

&lt;p&gt;The random search method is an alternative approach to hyperparameter tuning. It is pretty straightforward
in that it chooses a random set of hyperparameters within the chosen search space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pros&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Exploratory, you can give randomized search a wide distribution of hyperparameters to choose from 
to test out different search spaces.&lt;/li&gt;
  &lt;li&gt;Less likely to overfit&lt;/li&gt;
  &lt;li&gt;Not as computationally expensive as grid search&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cons&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lots more potential for variance due to its random nature&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s dive into it:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;# Import our packages
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomizedSearchCV&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Set up the model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Instead of setting discrete distributions of hyperparameters like we did with the logistic regression model, 
we can give continuous distributions for the randomized search method to explore:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;rf_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Range of integers from 4 to 204
&lt;/span&gt;    &lt;span class=&quot;s&quot;&gt;'n_estimators'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'max_features'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;auto&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;sqrt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;log2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Uniform distribution of values between 0.01 and 0.02
&lt;/span&gt;    &lt;span class=&quot;s&quot;&gt;'min_samples_split'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.199&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again check out the &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html&quot;&gt;docs&lt;/a&gt; on
random forest hyperparameters to get an understanding of which ones you may want to use.&lt;/p&gt;

&lt;p&gt;Laslty, we can implement them in our randomized search:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;random_search&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomizedSearchCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scoring&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rf_result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_search&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Best Score: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf_result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_score_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Best Hyperparameters: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf_result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_params_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the n_iter argument in RandomizedSearchCV can be set to run as many iterations of hyperparameter
tuning as you want.&lt;/p&gt;

&lt;p&gt;And our results as shown:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Best&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.776114081996435&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Best&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Hyperparameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'max_features'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'sqrt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'min_samples_split'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.02046703678839075&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'n_estimators'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;118&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;which-should-i-use&quot;&gt;Which Should I Use?&lt;/h4&gt;

&lt;p&gt;Both grid search and random search have their benefits and drawbacks as we have seen. In a nutshell, random seearch is
good for exploratory hyperparameter tuning over wider distributions and grid search is good for focusing on more narrow spaces.&lt;/p&gt;

&lt;p&gt;Thus, if time and computational power wasn’t an issue then you could use both to optimize a model. That being said,
let’s conclude and look at the last method: bayesian optimization.&lt;/p&gt;

&lt;h4 id=&quot;extra-bayesian-optimization&quot;&gt;Extra: Bayesian Optimization&lt;/h4&gt;

&lt;p&gt;On a final note, if any of you are interested in looking at the newest method of hyperparameter tuning, go check out bayesian optimization. 
Put simply, bayesian optimizers learn from each iteration of cross-validation and hone in on the most promising hyperparameters. It can often find better
hyperparameters in less iterations than grid search or random search. The packages &lt;a href=&quot;http://hyperopt.github.io/hyperopt/&quot;&gt;hyperopt&lt;/a&gt; and &lt;a href=&quot;https://optuna.org/&quot;&gt;optuna&lt;/a&gt; were made to implement this method in SKLearn. 
It is a bit more complex, but if you are up for it, go check out the docs and try to see if you can get one to work!&lt;/p&gt;

&lt;h4 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;As you have seen, hyperparameter tuning is very important if we want to have a working model that can effectively wrangle our data. 
Grid search and random search help us wade through the messy assortment of hyperparameters and search spaces so that we don’t have to do it ourselves.
Though we also see that data scientists are continuing to improve the methods with advanced approaches like bayesian optimization.&lt;/p&gt;

&lt;p&gt;Thanks for reading and if you have questions, please leave them below!&lt;/p&gt;</content><author><name></name></author><category term="" /><summary type="html">A Quickstart to Hyperparameter Tuning</summary></entry><entry><title type="html">Mastering Data Science Through Sports Betting</title><link href="/blog/Sports-Betting" rel="alternate" type="text/html" title="Mastering Data Science Through Sports Betting" /><published>2021-10-30T00:00:00-07:00</published><updated>2021-10-30T00:00:00-07:00</updated><id>/blog/Sports-Betting</id><content type="html" xml:base="/blog/Sports-Betting">&lt;p&gt;Okay, so you might be thinking that sports betting seems like a pretty frivolous application of data science. Perhaps it conjures up images of an addicted degenerate like the one Adam Sandler plays in &lt;em&gt;Uncut Gems&lt;/em&gt;, and adding statistics to it just makes it nerdier. While I can’t argue with the nerdy aspect, I do believe that sports betting, when used properly, can actually be a uniquely suited place for a burgeoning data scientist to learn and hone their skills. In this article I describe why I think that is, explain how you can get started, give an example of an actual successful bet powered by statistics, and list some of the best resources to learn more.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-30/sandler.png&quot; alt=&quot;sandler&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;first-off-what-is-sports-betting&quot;&gt;First off, what is sports betting?&lt;/h2&gt;

&lt;p&gt;To begin, let’s make sure we are all talking about the same thing. Sports betting is when you place a bet on the outcome of a sporting event. Oftentimes this can take the form of simply betting which team wins, but it can also be about the total amount of points scored in a game or even about the color of the Gatorade poured on the winning coach. Terms like the spread, the moneyline, and over/under all have to do with sports betting.&lt;/p&gt;

&lt;p&gt;Sports analytics on the other hand is a related but distinct field. In sports analytics, instead of betting on outcomes of a sporting event, you use data and statistical methods to make better strategic decisions to improve your odds of winning. Think Jonah Hill in Moneyball using statistics to guide their team. This is a very interesting field that has gained popularity in recent years, but it is not the subject of this article.&lt;/p&gt;

&lt;p&gt;Maybe the best way to make clear the distinction is that in general, if you are participating in the sporting event in some way (athlete, coach, etc.), you are likely to be using sports analytics, and if you are simply an observer making predictions, you are likely to be engaged in sports betting. The one exception to this would be NBA refs, who are obviously more interested in sports betting. The analogy to the business world would be the distinction between running a business (sports analytics) and investing in businesses (sports betting).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-30/jonah.png&quot; alt=&quot;jonah&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-is-sports-betting-a-uniquely-suited-place-to-practice-data-science&quot;&gt;Why is sports betting a uniquely suited place to practice data science?&lt;/h2&gt;

&lt;p&gt;Now that we have clearly defined what sports betting is and isn’t, we can talk about why I think it makes such a great playground for aspiring data scientists:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;For sports fans, it can be incredibly fun.&lt;/strong&gt; While for some having a deep understanding of the variables that affected a person’s chance of survival on the Titanic is fascinating, for many others it isn’t. Sports betting is a great way for people to use their passion and excitement for sports to fuel their statistical learning. Actually applying the skills learned in the classroom is absolutely essential to truly learn concepts, but if you don’t have exciting projects to work on, the chances that you will complete that crucial learning step on your own are very low. The sports betting world offers many projects that are naturally exciting and thus conducive to learning.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;It forces you to have skin in the game.&lt;/strong&gt; Data science is about using statistics to make decisions. Too often as learners we complete the first step (use statistics) without completing the crucial second step (actually make decisions). Decoupling these two steps hinders our development because if we don’t actually have something on the line with our conclusions, we will lack the rigor and practicality that comes through the natural learning process of feeling the high of being correct and the sting of being wrong. Actually risking something based on your conclusions will force you to understand more deeply the logic of what you are really doing in a way that is very hard to replicate otherwise. Nassim Taleb says this best:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Let us return to pathemata mathemata (learning through pain) and consider its reverse: learning through thrills and pleasure. People have two brains, one when there is skin in the game, one when there is none. Skin in the game can make boring things less boring. When you have skin in the game, dull things like checking the safety of the aircraft because you may be forced to be a passenger in it cease to be boring. If you are an investor in a company, doing ultra-boring things like reading the footnotes of a financial statement (where the real information is to be found) becomes, well, almost not boring.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;You get to actually do something about your findings.&lt;/strong&gt; In sports betting, when you have a conviction in an idea, it is very easy for you to implement it; you simply make a bet or a prediction. This is part of the reason why it can be so fun and why it is so unique. In other fields, especially as a student with limited resources, your opportunities to do something about your findings are limited. Even in sports analytics you can run all the analyses you want on going for it on fourth down, but ultimately the only thing you can do about it is pray Kalani agrees with you.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Gives you a chance to clearly communicate statistical ideas to non-statistical folk.&lt;/strong&gt; Communicating ideas clearly is always emphasized as one of the most important skills for a data scientist, but the problem is that in order to practice it, you have to have someone interested in hearing your explanations. I have never had so many non-statistics friends be really interested in me explaining to them logistic regression until the topic was the game that night and the $10 dollar bill in their wallet.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Tons of easily available information and constant opportunities for new projects.&lt;/strong&gt; Sports fans can be quite nerdy and so tons of data ready for analysis can be easily found online and there are new games all year round.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-do-i-get-started&quot;&gt;How do I get started?&lt;/h2&gt;

&lt;p&gt;Like anything, sports betting can be intimidating at first. Nevertheless, it doesn’t have to be complicated at all, and below I will walk through 3 simple steps to get you started using a simple project I worked on during the 2020 NBA playoffs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Qualitatively define your investment thesis by finding a difference in opinion.&lt;/strong&gt; When competing in markets, you can’t just be right; you have to be right where most others are wrong. For example, saying the Bucks are a great basketball team does nothing, but saying they are better than what most other people think they are could be the basis for a bet.&lt;/p&gt;

&lt;p&gt;In my example, I noticed that the odds for the player to score the first point in a game during the NBA playoffs seemed to be based off of the proportion of points each player scored for their team, but I believed that this was wrong and that the odds should actually be based off of the proportion of first points each player scored. I believed this because oftentimes teams will run set plays for certain players at the beginning of the game. My belief was based off a smaller sample size (only first points scored vs all points by starters), but I believed it would be more predictive regardless.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Calculate the expected value of a single bet based off of your unique probabilities.&lt;/strong&gt; If you have a difference in opinion, your implied probabilities will be different than those implied by the odds. This means you will likely see a bet with an expected value higher than 1x (getting more than 1 dollar back for every dollar you put in).&lt;/p&gt;

&lt;p&gt;In my case, I found my unique probabilities by scraping data for who scored the first points and then calculating the expected values using the published payouts. Below are the results for a game with the Lakers. As you can see, betting on Danny Green to score the first point had an expected value greater than 1x.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-30/ev.png&quot; alt=&quot;ev&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Determine the variance of your bet&lt;/strong&gt;. Just because a bet has an expected value above 1x does not mean it is a good bet because you may not be able to make enough similar bets to ensure you end up winning money most of the time with that strategy. For example, if you were offered a .1% chance at winning $10B dollars if you pay $1M, you would have an expected value of earning 10x your money, but you would only actually win money .1% of the time. The other 99.9% of the time you would lose $1M. If you could do this bet a large number of times, it would be worth it, but doing it only once is probably too risky for most.&lt;/p&gt;

&lt;p&gt;Applying this concept to my bet, I ran a Monte Carlo simulation with my calculated probability, first with 24 games played and then with 100 games played. As you can see, in both cases my returns are ~36% (as expected based on the expected value), but in the 100-game case I end up winning money overall much more often (81%) than in the 24-game case (52%). Thus, I only decided to proceed after verifying that I would be able to make bets with similar probabilities in a relatively high number of games. This is especially important for a strategy such as this where you expect to lose more often than you win but win big and lose little.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;games&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;24&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;226415&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;multiplier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;games&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bet&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiplier&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-30/mc.png&quot; alt=&quot;mc&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;where-can-i-go-to-learn-more&quot;&gt;Where can I go to learn more?&lt;/h2&gt;

&lt;p&gt;If this type of thing is exciting to you, there are many resources you can find online that are both entertaining and useful. Here are three that I have personally enjoyed:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://podcasts.apple.com/us/podcast/bet-the-process/id1291010585&quot;&gt;Podcast&lt;/a&gt; focused on data science in sports betting called &lt;em&gt;Bet the Process&lt;/em&gt;. Hosted by the real-life protagonist of the book/movie &lt;em&gt;21&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=FeOOpuDrZaQ&quot;&gt;Interview&lt;/a&gt; from DataCamp about the future of data science in sports betting and the unique innovations that are occurring in the space&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.justintodata.com/improve-sports-betting-odds-guide-in-python/&quot;&gt;Simple article&lt;/a&gt; detailing the practical steps for using python for sports betting&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Sports betting can be a fantastic way to train and test your data science skills. It offers real-world decision making, a variety of problem types, plenty of easily accessible data, and a whole lot of fun. In addition, the financialization of culture and the automatic market-making/smart-contract capabilities of blockchain technology (both topics for another post) mean there will be an incredible amount of interesting opportunities in the future and ensure that the sports betting hobby will only get better.&lt;/p&gt;

&lt;p&gt;Comment below if you have any interesting “investment theses” that could be worth looking into. It could be as simple as an idea for a collecting a new data type and thus having a proprietary predictor, applying a type of statistical model to a bet, or even just that the University of Utah will lose every football game for the rest of the season!&lt;/p&gt;

&lt;iframe src=&quot;https://giphy.com/embed/eHx5A2SWQtYRrG4WM2&quot; width=&quot;480&quot; height=&quot;270&quot; frameborder=&quot;0&quot; class=&quot;giphy-embed&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;Disclaimer: Sports betting can of course be considered a form of gambling and can be addictive. Though it is important to note that in several places sports betting is considered similar to investing, and you can even raise outside capital to form sports betting investment funds (zero correlation to the market is attractive). Nevertheless, considering the risks, it is totally possible to simply make “paper bets”, where you make predictions but don’t actually wager any money. You can still get the learning benefits the sports betting context provides without ever actually betting anything.&lt;/em&gt;&lt;/p&gt;</content><author><name>Noah Landers</name><email>noah.landers.13@gmail.com</email></author><category term="Sports Betting" /><category term="Sports Analytics" /><category term="Monte Carlo" /><category term="Skill Development" /><summary type="html">Okay, so you might be thinking that sports betting seems like a pretty frivolous application of data science. Perhaps it conjures up images of an addicted degenerate like the one Adam Sandler plays in Uncut Gems, and adding statistics to it just makes it nerdier. While I can’t argue with the nerdy aspect, I do believe that sports betting, when used properly, can actually be a uniquely suited place for a burgeoning data scientist to learn and hone their skills. In this article I describe why I think that is, explain how you can get started, give an example of an actual successful bet powered by statistics, and list some of the best resources to learn more. First off, what is sports betting? To begin, let’s make sure we are all talking about the same thing. Sports betting is when you place a bet on the outcome of a sporting event. Oftentimes this can take the form of simply betting which team wins, but it can also be about the total amount of points scored in a game or even about the color of the Gatorade poured on the winning coach. Terms like the spread, the moneyline, and over/under all have to do with sports betting. Sports analytics on the other hand is a related but distinct field. In sports analytics, instead of betting on outcomes of a sporting event, you use data and statistical methods to make better strategic decisions to improve your odds of winning. Think Jonah Hill in Moneyball using statistics to guide their team. This is a very interesting field that has gained popularity in recent years, but it is not the subject of this article. Maybe the best way to make clear the distinction is that in general, if you are participating in the sporting event in some way (athlete, coach, etc.), you are likely to be using sports analytics, and if you are simply an observer making predictions, you are likely to be engaged in sports betting. The one exception to this would be NBA refs, who are obviously more interested in sports betting. The analogy to the business world would be the distinction between running a business (sports analytics) and investing in businesses (sports betting). Why is sports betting a uniquely suited place to practice data science? Now that we have clearly defined what sports betting is and isn’t, we can talk about why I think it makes such a great playground for aspiring data scientists: For sports fans, it can be incredibly fun. While for some having a deep understanding of the variables that affected a person’s chance of survival on the Titanic is fascinating, for many others it isn’t. Sports betting is a great way for people to use their passion and excitement for sports to fuel their statistical learning. Actually applying the skills learned in the classroom is absolutely essential to truly learn concepts, but if you don’t have exciting projects to work on, the chances that you will complete that crucial learning step on your own are very low. The sports betting world offers many projects that are naturally exciting and thus conducive to learning. It forces you to have skin in the game. Data science is about using statistics to make decisions. Too often as learners we complete the first step (use statistics) without completing the crucial second step (actually make decisions). Decoupling these two steps hinders our development because if we don’t actually have something on the line with our conclusions, we will lack the rigor and practicality that comes through the natural learning process of feeling the high of being correct and the sting of being wrong. Actually risking something based on your conclusions will force you to understand more deeply the logic of what you are really doing in a way that is very hard to replicate otherwise. Nassim Taleb says this best: “Let us return to pathemata mathemata (learning through pain) and consider its reverse: learning through thrills and pleasure. People have two brains, one when there is skin in the game, one when there is none. Skin in the game can make boring things less boring. When you have skin in the game, dull things like checking the safety of the aircraft because you may be forced to be a passenger in it cease to be boring. If you are an investor in a company, doing ultra-boring things like reading the footnotes of a financial statement (where the real information is to be found) becomes, well, almost not boring.” You get to actually do something about your findings. In sports betting, when you have a conviction in an idea, it is very easy for you to implement it; you simply make a bet or a prediction. This is part of the reason why it can be so fun and why it is so unique. In other fields, especially as a student with limited resources, your opportunities to do something about your findings are limited. Even in sports analytics you can run all the analyses you want on going for it on fourth down, but ultimately the only thing you can do about it is pray Kalani agrees with you. Gives you a chance to clearly communicate statistical ideas to non-statistical folk. Communicating ideas clearly is always emphasized as one of the most important skills for a data scientist, but the problem is that in order to practice it, you have to have someone interested in hearing your explanations. I have never had so many non-statistics friends be really interested in me explaining to them logistic regression until the topic was the game that night and the $10 dollar bill in their wallet. Tons of easily available information and constant opportunities for new projects. Sports fans can be quite nerdy and so tons of data ready for analysis can be easily found online and there are new games all year round. How do I get started? Like anything, sports betting can be intimidating at first. Nevertheless, it doesn’t have to be complicated at all, and below I will walk through 3 simple steps to get you started using a simple project I worked on during the 2020 NBA playoffs. 1. Qualitatively define your investment thesis by finding a difference in opinion. When competing in markets, you can’t just be right; you have to be right where most others are wrong. For example, saying the Bucks are a great basketball team does nothing, but saying they are better than what most other people think they are could be the basis for a bet. In my example, I noticed that the odds for the player to score the first point in a game during the NBA playoffs seemed to be based off of the proportion of points each player scored for their team, but I believed that this was wrong and that the odds should actually be based off of the proportion of first points each player scored. I believed this because oftentimes teams will run set plays for certain players at the beginning of the game. My belief was based off a smaller sample size (only first points scored vs all points by starters), but I believed it would be more predictive regardless. 2. Calculate the expected value of a single bet based off of your unique probabilities. If you have a difference in opinion, your implied probabilities will be different than those implied by the odds. This means you will likely see a bet with an expected value higher than 1x (getting more than 1 dollar back for every dollar you put in). In my case, I found my unique probabilities by scraping data for who scored the first points and then calculating the expected values using the published payouts. Below are the results for a game with the Lakers. As you can see, betting on Danny Green to score the first point had an expected value greater than 1x. 3. Determine the variance of your bet. Just because a bet has an expected value above 1x does not mean it is a good bet because you may not be able to make enough similar bets to ensure you end up winning money most of the time with that strategy. For example, if you were offered a .1% chance at winning $10B dollars if you pay $1M, you would have an expected value of earning 10x your money, but you would only actually win money .1% of the time. The other 99.9% of the time you would lose $1M. If you could do this bet a large number of times, it would be worth it, but doing it only once is probably too risky for most. Applying this concept to my bet, I ran a Monte Carlo simulation with my calculated probability, first with 24 games played and then with 100 games played. As you can see, in both cases my returns are ~36% (as expected based on the expected value), but in the 100-game case I end up winning money overall much more often (81%) than in the 24-game case (52%). Thus, I only decided to proceed after verifying that I would be able to make bets with similar probabilities in a relatively high number of games. This is especially important for a strategy such as this where you expect to lose more often than you win but win big and lose little. games = 24 prob = .226415/2 bet = 100 multiplier = 11 n = 100000 total = [] for i in range(n): score = 0 for i in range(games): value = random.choices([0,1], weights=[1-prob, prob], k=1)[0] if value == 0: score -= bet elif value == 1: score += bet*multiplier total.append(score) Where can I go to learn more? If this type of thing is exciting to you, there are many resources you can find online that are both entertaining and useful. Here are three that I have personally enjoyed: Podcast focused on data science in sports betting called Bet the Process. Hosted by the real-life protagonist of the book/movie 21 Interview from DataCamp about the future of data science in sports betting and the unique innovations that are occurring in the space Simple article detailing the practical steps for using python for sports betting Conclusion Sports betting can be a fantastic way to train and test your data science skills. It offers real-world decision making, a variety of problem types, plenty of easily accessible data, and a whole lot of fun. In addition, the financialization of culture and the automatic market-making/smart-contract capabilities of blockchain technology (both topics for another post) mean there will be an incredible amount of interesting opportunities in the future and ensure that the sports betting hobby will only get better. Comment below if you have any interesting “investment theses” that could be worth looking into. It could be as simple as an idea for a collecting a new data type and thus having a proprietary predictor, applying a type of statistical model to a bet, or even just that the University of Utah will lose every football game for the rest of the season! Disclaimer: Sports betting can of course be considered a form of gambling and can be addictive. Though it is important to note that in several places sports betting is considered similar to investing, and you can even raise outside capital to form sports betting investment funds (zero correlation to the market is attractive). Nevertheless, considering the risks, it is totally possible to simply make “paper bets”, where you make predictions but don’t actually wager any money. You can still get the learning benefits the sports betting context provides without ever actually betting anything.</summary></entry><entry><title type="html">The Importance of Play Type and Location in the NFL</title><link href="/blog/Football-Statistical-Analysis" rel="alternate" type="text/html" title="The Importance of Play Type and Location in the NFL" /><published>2021-10-29T00:00:00-07:00</published><updated>2021-10-29T00:00:00-07:00</updated><id>/blog/Football-Statistical-Analysis</id><content type="html" xml:base="/blog/Football-Statistical-Analysis">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Data analytics have become an increasingly prevalent tool used to help make important decisions in the field of professional sports.  In particular, the National Football League has implemented a wide variety of data analytic methods to aid in evaluating NFL draft prospects.  Similarly, this study aims to use data analytic methods to evaluate the relationship between type of play, play location, and yards gained, and whether any of these relationships are statistically significant. When attempting to determine whether results are significant, certain assumptions have to be met in order to use standard parametric methods, such as data being homoscedastic and following a Gaussian distribution.  However, what can we do if our data does not meet these assumptions?  For cases such as this we can turn to non-parametric methods in order to determine statistical significance. The number of yards gained (or lost) for each field location and type of play was compared through both parametric and non-parametric methods.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;In order to determine which play location and type of play are the best, this study compares the yards gained for each type of play, as well as the yards gained for each play location.  The dataset was obtained from Kaggle.com, and contains a plethora of information pertaining to the National Football League.  This data was filtered down to three pertinent columns: “Yards.Gained”, “RunLocation”, and “PassLocation”, from which further statistical analysis was performed.  This statistical analysis will include both parametric and non-parametric tests, and the results of both types of tests will be compared to each other in order to draw the most accurate conclusions possible.  For the parametric methods, a two sample t-test was used to compare the mean yards gained for each type of play (run versus pass), as well as a one-way analysis of variance (ANOVA) was performed to compare the mean yards gained for each type of play location (left versus middle versus right).  For these parametric tests to be conducted, certain assumptions are made, specifically that all data points follow a Gaussian distribution, and all points must have similar variances. If these assumptions were not met, then non-parametric methods were performed as well.  The non-parametric equivalents to the parametric test we use is a Mann-Whitney U test for the two sample t-test, and a Kruskal-Wallis one-way analysis of variance for ANOVA.&lt;/p&gt;

&lt;p&gt;The null hypothesis for the two sample t-test and the Mann-Whitney U test is that the mean number of yards gained per run play is equal to the mean number of yards gained per pass play.  The alternative hypothesis for these tests will be that there is a difference between these two groups, and the threshold for this statistical significance is an alpha level of 0.05.  Additionally, the null hypothesis for the ANOVA and the Kruskal-Wallis test is that there is no difference between the mean number of yards gained for each run location, and the alternative hypothesis will be that there is a difference between any of the three means, with an alpha level of 0.05, the same as the previous tests.  The results of each test were compared to its parametric or non-parametric counterpart, and these results were then interpreted in conjunction with each other.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;exploratory-data-analysis&quot;&gt;Exploratory Data Analysis&lt;/h2&gt;

&lt;h3 id=&quot;run-location&quot;&gt;Run Location&lt;/h3&gt;

&lt;p&gt;Looking at the summary statistics for yards gained by run location, we see that the means appear to differ by a small amount, though there are two outliers that could explain these differences. The medians and standard deviations are also different, in particular the median and standard deviation pertaining to runs in the middle location compared to the other run locations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-29/runlocationinfo.png&quot; alt=&quot;runinfo&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;pass-location&quot;&gt;Pass Location&lt;/h3&gt;

&lt;p&gt;The summary statistics for yards gained by pass location differ from each other as well, similar to the yards gained by run location. Passes from the left location seem to especially differ from the other pass location, with the mean and standard deviation both being close to twice as large as the respective measurements for the other pass locations, though this could be due to the two outliers present in the data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-29/passlocationinfo.png&quot; alt=&quot;passinfo&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;play-type&quot;&gt;Play Type&lt;/h3&gt;

&lt;p&gt;Finally, we can see that the summary statistics comparing play type show a clear difference in means, standard deviations, and medians. However, this difference is largely attributed to the two outliers present in the data, with the maximum yards gained for passes being 43 yards, and the maximum yards gained for runs being 28 yards.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-29/playtypeinfo.png&quot; alt=&quot;playinfo&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;q-q-plots&quot;&gt;Q-Q Plots&lt;/h3&gt;

&lt;p&gt;After looking at the summary statistics of the data, we can now check for normality through the use of Q-Q plots. The data looks normal, but due to the small sample size, the results are inconclusive. We can also check for skewness, and after doing so, can see that the data based on run location and the data based on pass location are both fairly skewed, so we can perform a variety of data transformations in order to attempt to remedy this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-29/qqplots.png&quot; alt=&quot;qqplots&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By looking at the skewness for each possible data transformation, we can see that the fourth root transformation and the square root transformation bring the skewness values closer to zero than any other transformation for run data and pass data, respectively. After seeing this, we can now view Q-Q plots of the transformed data and see that plots appear to be more normal than the pre-transformed data. Going forward, we will use this transformed data when conducting our two sample t-tests and ANOVA, as the transformed data better adheres to the assumptions necessary to do so. For the non-parametric methods of Mann-Whitney U test and Kruskal-Wallis test, there is no need to use the transformed data, as the assumptions needed to perform parametric tests do not need to be met.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-29/transformedqqplot.png&quot; alt=&quot;transqqplots&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;permutation-tests&quot;&gt;Permutation Tests&lt;/h3&gt;

&lt;p&gt;Another way that we can calculate the p-value for our tests of significance is through permutation tests. For comparing yards gained versus play location for both run and pass plays, we create a function that takes our sample data and the amount of groups we wish to divide the the sample data into as parameters, which in this case is three groups, as the amount of different play locations is equal to three. Our function then randomly divides the data into the desired amount of groups and conducts a Kruskal-Wallis test on the newly created groups, finally returning the calculated chi-squared statistic. For the permutation test, we run this function a total of 100000 times and save the results as a variable, as this creates a list of 100000 calculated chi-squared statistics. We then view the frequency of our chi-squared statistics in a histogram and see the proportion of calculated statistics that are greater than the chi-squared statistic we calculated from our original, non-shuffled data. This proportion gives us an estimate for our p-value, which we then compare to the p-value from the Kruskal-Wallis test on our original dataset.&lt;/p&gt;

&lt;p&gt;Similarly, when comparing yards gained versus play type, we create a function that accepts our dataset as a parameter, and then randomly divides this dataset into two equal sized groups. A Wilcox test is then performed on the two resulting groups and the calculated w-statistic is returned as the result. Like the previous permutation test, this function is run 100000 times and the calculated statistics are saved as a list. This list is then displayed as a histogram, where we see the proportion of w-statistics that are as extreme or more extreme (in this case, less than or equal to) than the w-statistic we calculated for our original dataset. This gives us the p-value for a one-sided test of significance, but we need to find the p-value for a two-sided test of significance. In order to find the desired p-value, we find the quantile for our original dataset’s w-statistic and then find the corresponding quantile on the opposite side of our histogram, which is equal to 1 minus our first quantile value. We then find the proportion of w-statistics that are as extreme or more extreme (in this case, greater than or equal to) than the original dataset’s w-statistic and add this proportion to the first proportion we calculated, which finally gives us the p-value for our two-sided test of significance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-29/permtests.png&quot; alt=&quot;permtests&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above plots help to illustrate how our permutation tests work. The histograms show the spread of the calculated statistics for both our Kruskal-Wallis permutation test and our Wilcox permutation test, and the vertical lines on each plot show the value of the statistics we calculated for our original dataset. For our Kruskal-Wallis permutations, the area to the right of our vertical line is equal to our p-value, as this is the probability of obtaining the chi-squared statistic we got or one more extreme. For our Wilcox permutations, the two vertical lines correspond to the quantiles we calculated earlier, and the p-value is equal to the area to the left of the first line plus the area to the right of our second line. For all of the above plots, the p-value we obtained from our permutation tests is labeled on the plot and is discussed further in the results section of this report.&lt;/p&gt;

&lt;h3 id=&quot;bootstrap-methods&quot;&gt;Bootstrap Methods&lt;/h3&gt;

&lt;p&gt;For our bootstrap methods, we create two functions that are nearly identical to the previously created permutation test functions. However, the one difference between our bootstrapping functions and our permutation test functions is found in how we sample our data. For the permutation tests, we sample from the original dataset without replacement, whereas for bootstrap methods, we sample from the original dataset with replacement. After changing our functions accordingly, we run the functions 100000 times for both our Kruskal-Wallis bootstrap method and our Wilcox bootstrap method and generate lists of the statistics we calculated from these methods. We then determine the p-values from these tests in the same way as we did for our permutation tests, and then compare these results with the results we obtained from the tests we conducted on our original dataset as well as the results from our permutation tests.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-29/bootstrapmethods.png&quot; alt=&quot;bootmethods&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see from the previous plots, the obtained p-values are very similar to those we obtained from our permutation tests, as well as the parametric tests and the non-parametric tests we conducted on our original dataset. The p-value for a specific test is the probability of obtaining a single test statistic or one more extreme than what we have, and we can see how this concept is illustrated in our plots. For
our Kruskal-Wallis bootstrap methods, the p-value is equal to the area to the right of the vertical line on our plots, as this line represents the test statistics calculated for our original dataset. For our Wilcox test bootstrap method, the p-value is equal to the area moving away from the mean on either side of the two vertical lines in the plot, as these lines represent the appropriate quantiles for a two-sided test of significance.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The results of the run location data analysis show that the p-value for the ANOVA test is 0.4085286, which is higher than our alpha of 0.05. The Kruskal-Wallis test returned a p-value of 0.169678, which further reinforces that our null hypothesis cannot be rejected: there is no difference in the average amount of yards gained based on run location. Additionally, our permutation test and bootstrap methods for run location returned p-values of 0.51042 and 0.51311 respectively, which are both p-values that result in us failing to reject the null hypothesis.&lt;/p&gt;

&lt;p&gt;The results based on the pass location data analysis are similar to those based on the run location. The pvalue for the ANOVA test is 0.2928736 and the p-value for the Kruskal-Wallis test is equal to 0.5012825, which are both higher than the predetermined threshold of an alpha level of 0.05. In addition, our permutation test and bootstrap methods for pass location resulted in p-values of 0.16931 and 0.16907 respectively. The
p-values from these four type of tests lead to the conclusion that our null hypothesis cannot be rejected.&lt;/p&gt;

&lt;p&gt;Finally, the results of our play type data analysis show that there is a statistically significant difference between the average number of yards gained per run play and the average number of yards gained per pass play. The p-value of the two sample t-test is 0.0279695, and the p-value from the Mann-Whitney U test is equal to 0.0237371, which both support rejecting the null hypothesis and using the alternative hypothesis. This conclusion is further supported by the permutation test and bootstrap methods for play type, as the returned p-values are 0.02202 and 0.02132 respectively, with both of these p-values being below our alpha threshold of 0.05.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Based on the previously stated results, we see that although the location of a play does not make a difference, the type of play conducted does make a difference. This information could prove invaluable to any and all teams belonging to the National Football League, as it could influence a wide variety of decisions each team makes. Because pass plays results in a significantly larger average amount of yards gained than run plays, teams looking to improve could allocate more of their resources into improving their quarterbacks and wide receivers, as well as any other individuals involved in passing plays. Additionally, this information can influence which players are prioritized in the NFL Draft, as it could place an increased emphasis on drafting players that specialize in conducting passing plays. Alternatively, because the location of a play was found to not be significant, teams do not need to consider whether their players have a preference for a certain location on the field when drawing up a game plan, such as a left-handed versus a right-handed quarterback.&lt;/p&gt;

&lt;p&gt;The dataset used for this analysis was too small to effectively check that the assumptions for equal variance and normality were met; however, because both the parametric methods and non-parametric methods agreed with each other, we can conclude that is not worth using non-parametric tests in lieu of parametric tests for this dataset, due to the loss of power associated with conducting non-parametric tests. Despite reaching these conclusions, we should consider how these conclusions may have changed if the sample size had increased. Would the data follow a Gaussian distribution and/or would the resulting conclusions have differed? Finally, though the difference in yards gained by pass plays and run plays was found to be statistically significant, how practically significant does this result turn out to be?&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;future-recommendations&quot;&gt;Future Recommendations&lt;/h2&gt;

&lt;p&gt;Though this study may have found a difference in play type yard gain (as well as not finding a difference in play location), the dataset the study was based on contained information based on all NFL teams, which may not necessarily apply to every individual team. If a specific team wanted to determine how these results/conclusions apply to themselves, it would be important to obtain a sample solely based on data from that specific team. Additionally, comparing the average amount of yards gained based on other variables would allow us to draw more accurate conclusions, as the differences found in the analysis could have possibly been influenced by any number of confounding variables.&lt;/p&gt;</content><author><name>Brad Hymas</name><email>hymasbrad@gmail.com</email></author><category term="R" /><category term="Non-Parametric Methods" /><category term="Sports Analytics" /><summary type="html">Introduction Data analytics have become an increasingly prevalent tool used to help make important decisions in the field of professional sports. In particular, the National Football League has implemented a wide variety of data analytic methods to aid in evaluating NFL draft prospects. Similarly, this study aims to use data analytic methods to evaluate the relationship between type of play, play location, and yards gained, and whether any of these relationships are statistically significant. When attempting to determine whether results are significant, certain assumptions have to be met in order to use standard parametric methods, such as data being homoscedastic and following a Gaussian distribution. However, what can we do if our data does not meet these assumptions? For cases such as this we can turn to non-parametric methods in order to determine statistical significance. The number of yards gained (or lost) for each field location and type of play was compared through both parametric and non-parametric methods. Overview In order to determine which play location and type of play are the best, this study compares the yards gained for each type of play, as well as the yards gained for each play location. The dataset was obtained from Kaggle.com, and contains a plethora of information pertaining to the National Football League. This data was filtered down to three pertinent columns: “Yards.Gained”, “RunLocation”, and “PassLocation”, from which further statistical analysis was performed. This statistical analysis will include both parametric and non-parametric tests, and the results of both types of tests will be compared to each other in order to draw the most accurate conclusions possible. For the parametric methods, a two sample t-test was used to compare the mean yards gained for each type of play (run versus pass), as well as a one-way analysis of variance (ANOVA) was performed to compare the mean yards gained for each type of play location (left versus middle versus right). For these parametric tests to be conducted, certain assumptions are made, specifically that all data points follow a Gaussian distribution, and all points must have similar variances. If these assumptions were not met, then non-parametric methods were performed as well. The non-parametric equivalents to the parametric test we use is a Mann-Whitney U test for the two sample t-test, and a Kruskal-Wallis one-way analysis of variance for ANOVA. The null hypothesis for the two sample t-test and the Mann-Whitney U test is that the mean number of yards gained per run play is equal to the mean number of yards gained per pass play. The alternative hypothesis for these tests will be that there is a difference between these two groups, and the threshold for this statistical significance is an alpha level of 0.05. Additionally, the null hypothesis for the ANOVA and the Kruskal-Wallis test is that there is no difference between the mean number of yards gained for each run location, and the alternative hypothesis will be that there is a difference between any of the three means, with an alpha level of 0.05, the same as the previous tests. The results of each test were compared to its parametric or non-parametric counterpart, and these results were then interpreted in conjunction with each other. Exploratory Data Analysis Run Location Looking at the summary statistics for yards gained by run location, we see that the means appear to differ by a small amount, though there are two outliers that could explain these differences. The medians and standard deviations are also different, in particular the median and standard deviation pertaining to runs in the middle location compared to the other run locations. Pass Location The summary statistics for yards gained by pass location differ from each other as well, similar to the yards gained by run location. Passes from the left location seem to especially differ from the other pass location, with the mean and standard deviation both being close to twice as large as the respective measurements for the other pass locations, though this could be due to the two outliers present in the data. Play Type Finally, we can see that the summary statistics comparing play type show a clear difference in means, standard deviations, and medians. However, this difference is largely attributed to the two outliers present in the data, with the maximum yards gained for passes being 43 yards, and the maximum yards gained for runs being 28 yards. Q-Q Plots After looking at the summary statistics of the data, we can now check for normality through the use of Q-Q plots. The data looks normal, but due to the small sample size, the results are inconclusive. We can also check for skewness, and after doing so, can see that the data based on run location and the data based on pass location are both fairly skewed, so we can perform a variety of data transformations in order to attempt to remedy this. By looking at the skewness for each possible data transformation, we can see that the fourth root transformation and the square root transformation bring the skewness values closer to zero than any other transformation for run data and pass data, respectively. After seeing this, we can now view Q-Q plots of the transformed data and see that plots appear to be more normal than the pre-transformed data. Going forward, we will use this transformed data when conducting our two sample t-tests and ANOVA, as the transformed data better adheres to the assumptions necessary to do so. For the non-parametric methods of Mann-Whitney U test and Kruskal-Wallis test, there is no need to use the transformed data, as the assumptions needed to perform parametric tests do not need to be met. Permutation Tests Another way that we can calculate the p-value for our tests of significance is through permutation tests. For comparing yards gained versus play location for both run and pass plays, we create a function that takes our sample data and the amount of groups we wish to divide the the sample data into as parameters, which in this case is three groups, as the amount of different play locations is equal to three. Our function then randomly divides the data into the desired amount of groups and conducts a Kruskal-Wallis test on the newly created groups, finally returning the calculated chi-squared statistic. For the permutation test, we run this function a total of 100000 times and save the results as a variable, as this creates a list of 100000 calculated chi-squared statistics. We then view the frequency of our chi-squared statistics in a histogram and see the proportion of calculated statistics that are greater than the chi-squared statistic we calculated from our original, non-shuffled data. This proportion gives us an estimate for our p-value, which we then compare to the p-value from the Kruskal-Wallis test on our original dataset. Similarly, when comparing yards gained versus play type, we create a function that accepts our dataset as a parameter, and then randomly divides this dataset into two equal sized groups. A Wilcox test is then performed on the two resulting groups and the calculated w-statistic is returned as the result. Like the previous permutation test, this function is run 100000 times and the calculated statistics are saved as a list. This list is then displayed as a histogram, where we see the proportion of w-statistics that are as extreme or more extreme (in this case, less than or equal to) than the w-statistic we calculated for our original dataset. This gives us the p-value for a one-sided test of significance, but we need to find the p-value for a two-sided test of significance. In order to find the desired p-value, we find the quantile for our original dataset’s w-statistic and then find the corresponding quantile on the opposite side of our histogram, which is equal to 1 minus our first quantile value. We then find the proportion of w-statistics that are as extreme or more extreme (in this case, greater than or equal to) than the original dataset’s w-statistic and add this proportion to the first proportion we calculated, which finally gives us the p-value for our two-sided test of significance. The above plots help to illustrate how our permutation tests work. The histograms show the spread of the calculated statistics for both our Kruskal-Wallis permutation test and our Wilcox permutation test, and the vertical lines on each plot show the value of the statistics we calculated for our original dataset. For our Kruskal-Wallis permutations, the area to the right of our vertical line is equal to our p-value, as this is the probability of obtaining the chi-squared statistic we got or one more extreme. For our Wilcox permutations, the two vertical lines correspond to the quantiles we calculated earlier, and the p-value is equal to the area to the left of the first line plus the area to the right of our second line. For all of the above plots, the p-value we obtained from our permutation tests is labeled on the plot and is discussed further in the results section of this report. Bootstrap Methods For our bootstrap methods, we create two functions that are nearly identical to the previously created permutation test functions. However, the one difference between our bootstrapping functions and our permutation test functions is found in how we sample our data. For the permutation tests, we sample from the original dataset without replacement, whereas for bootstrap methods, we sample from the original dataset with replacement. After changing our functions accordingly, we run the functions 100000 times for both our Kruskal-Wallis bootstrap method and our Wilcox bootstrap method and generate lists of the statistics we calculated from these methods. We then determine the p-values from these tests in the same way as we did for our permutation tests, and then compare these results with the results we obtained from the tests we conducted on our original dataset as well as the results from our permutation tests. As we can see from the previous plots, the obtained p-values are very similar to those we obtained from our permutation tests, as well as the parametric tests and the non-parametric tests we conducted on our original dataset. The p-value for a specific test is the probability of obtaining a single test statistic or one more extreme than what we have, and we can see how this concept is illustrated in our plots. For our Kruskal-Wallis bootstrap methods, the p-value is equal to the area to the right of the vertical line on our plots, as this line represents the test statistics calculated for our original dataset. For our Wilcox test bootstrap method, the p-value is equal to the area moving away from the mean on either side of the two vertical lines in the plot, as these lines represent the appropriate quantiles for a two-sided test of significance. Results The results of the run location data analysis show that the p-value for the ANOVA test is 0.4085286, which is higher than our alpha of 0.05. The Kruskal-Wallis test returned a p-value of 0.169678, which further reinforces that our null hypothesis cannot be rejected: there is no difference in the average amount of yards gained based on run location. Additionally, our permutation test and bootstrap methods for run location returned p-values of 0.51042 and 0.51311 respectively, which are both p-values that result in us failing to reject the null hypothesis. The results based on the pass location data analysis are similar to those based on the run location. The pvalue for the ANOVA test is 0.2928736 and the p-value for the Kruskal-Wallis test is equal to 0.5012825, which are both higher than the predetermined threshold of an alpha level of 0.05. In addition, our permutation test and bootstrap methods for pass location resulted in p-values of 0.16931 and 0.16907 respectively. The p-values from these four type of tests lead to the conclusion that our null hypothesis cannot be rejected. Finally, the results of our play type data analysis show that there is a statistically significant difference between the average number of yards gained per run play and the average number of yards gained per pass play. The p-value of the two sample t-test is 0.0279695, and the p-value from the Mann-Whitney U test is equal to 0.0237371, which both support rejecting the null hypothesis and using the alternative hypothesis. This conclusion is further supported by the permutation test and bootstrap methods for play type, as the returned p-values are 0.02202 and 0.02132 respectively, with both of these p-values being below our alpha threshold of 0.05. Conclusion Based on the previously stated results, we see that although the location of a play does not make a difference, the type of play conducted does make a difference. This information could prove invaluable to any and all teams belonging to the National Football League, as it could influence a wide variety of decisions each team makes. Because pass plays results in a significantly larger average amount of yards gained than run plays, teams looking to improve could allocate more of their resources into improving their quarterbacks and wide receivers, as well as any other individuals involved in passing plays. Additionally, this information can influence which players are prioritized in the NFL Draft, as it could place an increased emphasis on drafting players that specialize in conducting passing plays. Alternatively, because the location of a play was found to not be significant, teams do not need to consider whether their players have a preference for a certain location on the field when drawing up a game plan, such as a left-handed versus a right-handed quarterback. The dataset used for this analysis was too small to effectively check that the assumptions for equal variance and normality were met; however, because both the parametric methods and non-parametric methods agreed with each other, we can conclude that is not worth using non-parametric tests in lieu of parametric tests for this dataset, due to the loss of power associated with conducting non-parametric tests. Despite reaching these conclusions, we should consider how these conclusions may have changed if the sample size had increased. Would the data follow a Gaussian distribution and/or would the resulting conclusions have differed? Finally, though the difference in yards gained by pass plays and run plays was found to be statistically significant, how practically significant does this result turn out to be? Future Recommendations Though this study may have found a difference in play type yard gain (as well as not finding a difference in play location), the dataset the study was based on contained information based on all NFL teams, which may not necessarily apply to every individual team. If a specific team wanted to determine how these results/conclusions apply to themselves, it would be important to obtain a sample solely based on data from that specific team. Additionally, comparing the average amount of yards gained based on other variables would allow us to draw more accurate conclusions, as the differences found in the analysis could have possibly been influenced by any number of confounding variables.</summary></entry><entry><title type="html">How to Make Graphics Stand Out Using Plotly</title><link href="/blog/plotyly" rel="alternate" type="text/html" title="How to Make Graphics Stand Out Using Plotly" /><published>2021-10-28T00:00:00-07:00</published><updated>2021-10-28T00:00:00-07:00</updated><id>/blog/plotyly</id><content type="html" xml:base="/blog/plotyly">&lt;p&gt;Many disciplines, including statistics, finance, and business, use data visualization to convey information quickly and efficiently to their audience. As such, there are many libraries and packages that have been created to help people make all sorts of plots and graphs. Plotly is a data visualization library that you can use to create beautiful and interactive graphs and is supported in different programing languages such as Python, R, and Julia. It has become a popular library for data visualization because it is easy to implement code, makes visually appealing graphics, and the plots are user interactive. These plots are especially useful during exploratory data analysis, where you can create a graphic and then click around and look at individual points. It is also a great tool to share with your audience, who can interact with the graph themselves.&lt;/p&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;

&lt;p&gt;If you are using Python, plotly many be installed using pip or pip 3:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plotly&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or you can install using conda:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;conda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plotly&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plotly&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;boxplots&quot;&gt;Boxplots&lt;/h2&gt;

&lt;p&gt;One of the basics of data graphics is the boxplot. Anytime we want to use plotly, we can call the library at the beginning of our session. The standard way to do this is:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plotly.express&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Where “px” will be the function you use to create graphs. For these examples I will be pulling data from a data frame that is built into python, so you can try the code out yourselves.  Using the well-known iris dataset built into python, we can create a boxplot at each sepal length of sepal width:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plotly.express&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;box&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sepal_length&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;petal_length&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-28/newplot.png&quot; alt=&quot;MySQL&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A graphic like the one above will appear. In your window, you will be able to interact with it by hovering over a single sepal length’s boxplot, and it will tell you things like: min, max, q3, q4, and median.&lt;/p&gt;

&lt;h2 id=&quot;bar-graphs&quot;&gt;Bar Graphs&lt;/h2&gt;

&lt;p&gt;Another basic but important tool in plotly is px.bar, which generates bar graphs. We define the data frame, and then the x and y variables we want to look at. One really cool feature here, is you can add another layer of information to your graph by color-coding it by a third variable. you can do this by using the color argument shown below. In the example code, we color coded by what species the flower is. Plotly will automatically create a legend for what color is what species.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sepal_width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sepal_length&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;species&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Bar Graph&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-28/Bar.png&quot; alt=&quot;MySQL&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the graph, you will be able to hover over each point and see the breakdown of species for each sepal width.&lt;/p&gt;

&lt;h2 id=&quot;scatterplots&quot;&gt;Scatterplots&lt;/h2&gt;

&lt;p&gt;Another nice feature you can take advantage of is scatterplots. You use the command px.scatter, and then define your x and y variables. We again use the color argument to add another level of information to our graph. In the example code, we color coded by the same variable as we did above, which is species.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sepal_width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sepal_length&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;species&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A Plotly Express Figure&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-28/Scatter.png&quot; alt=&quot;MySQL&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;adding-a-trendline&quot;&gt;Adding a trendline&lt;/h2&gt;
&lt;p&gt;We can add a trendline to our scatterplot using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt; trendline=&quot;ols&quot; &amp;gt;&lt;/code&gt; arguement. It will make a trendline for each species, since we keep the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt; color=&quot;species&quot; &amp;gt;&lt;/code&gt; argument&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sepal_width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sepal_length&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;species&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trendline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ols&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A Plotly Express Figure&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-28/trendline.png&quot; alt=&quot;MySQL&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;posting-your-plotly-graphic-online&quot;&gt;Posting your Plotly Graphic Online&lt;/h2&gt;

&lt;p&gt;We have gone over a few simple functions in plotly that will create different graphs to look at our dataset. We will be able to interact with the plot ourselves, but what if you want to share your plotly graphic with others?&lt;/p&gt;

&lt;p&gt;You can do this pretty easily if you have a plotly account. If you don’t already have one, make an account with plotly at plot.ly. Then you can install chart_studios with the code below.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chart_studio&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then, using the username and api password that you get with your plotly account, run the following code, where fig is the name of the figure you made, and filename is the name you want to give your plot.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;chart_studio&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;username&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;''&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# your username
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;''&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# your api key - go to profile &amp;gt; settings &amp;gt; regenerate key
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chart_studio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_credentials_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;chart_studio.plotly&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Trendline'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto_open&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After doing this, you should automatically be taken to a link in your account where you can see your interactive graph.&lt;/p&gt;

&lt;p&gt;https://chart-studio.plotly.com/~bethanybassett/1/#/&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The plotly library is almost limitless, and can generate almost any kind of chart or plot you can think of. This post is only an introduction into what plotly can do. Some of the other graphs that I think are worth looking into are line charts, pie charts, bubble charts, sankey diagrams, sunburst charts, and many more! There is great documentation beyond this post that you can find on plotly’s site that will help you develop your graphic skills. Post below in the comment section examples where you have successfully used plotly, and if possible, attach pictures or code to go with it. It would be especially interesting to see types of graphs that we were not able to cover in this blog post. Good luck!&lt;/p&gt;</content><author><name>Bethany Bassett</name><email>bethanysday@gmail.com</email></author><category term="Learning" /><category term="Python" /><category term="Data Visualization" /><summary type="html">Many disciplines, including statistics, finance, and business, use data visualization to convey information quickly and efficiently to their audience. As such, there are many libraries and packages that have been created to help people make all sorts of plots and graphs. Plotly is a data visualization library that you can use to create beautiful and interactive graphs and is supported in different programing languages such as Python, R, and Julia. It has become a popular library for data visualization because it is easy to implement code, makes visually appealing graphics, and the plots are user interactive. These plots are especially useful during exploratory data analysis, where you can create a graphic and then click around and look at individual points. It is also a great tool to share with your audience, who can interact with the graph themselves.</summary></entry><entry><title type="html">See More with Seaborn</title><link href="/blog/Seaborn" rel="alternate" type="text/html" title="See More with Seaborn" /><published>2021-10-26T00:00:00-07:00</published><updated>2021-10-26T00:00:00-07:00</updated><id>/blog/Seaborn</id><content type="html" xml:base="/blog/Seaborn">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;The Seaborn library is a Python package which creates data visualizations based on the matplotlib library. As a result, this blog post will assume that the reader has a basic understanding of the matplotlib library. The main draw to the Seaborn library is its ability to create more visually appealing data plots than the matplotlib. Additionally, Seaborn plots contain more attributes which allow plots to contain more characteristics than their matplotlib counterparts.&lt;/p&gt;

&lt;p&gt;Throughout this blog post, we will go over a few different plots and the characteristics available through the Seaborn library. Of course this post will by no means be exhaustive. Thus, the reader is encouraged to delve further on their own for more information regarding the remarkable abilities of Seaborn plots. For those interested more examples can be found at https://seaborn.pydata.org/examples/index.html which displays a variety of various plots some of which the reader may be unaware exists.&lt;/p&gt;

&lt;h1 id=&quot;setting-up&quot;&gt;Setting Up&lt;/h1&gt;

&lt;p&gt;For this post I will be using the diamonds dataset which comes with the Seaborn library. There are a number of datasets which come with Seaborn which can be used for exploring Seaborn plots, one can Google the Seaborn datasets if the reader desires to explore the datasets. If one desires to use a Seaborn dataset they can load the data using the following code: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sns.load_dataset(&quot;dataset&quot;)&lt;/code&gt;, otherwise load in the data using other methods the reader is familiar with.&lt;/p&gt;

&lt;p&gt;To the observant reader, one can deduce that the Python shortcut for Seaborn is sns, thus, loading the Seaborn library would be as follows:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import seaborn as sns
import matplotlib.pyplot as plt

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It is usual to also include the matplotlib plot library when using seaborn since both libraries can be used together. In fact matplotlib is used for plot titles, legends, and other custom alterations to the plot.&lt;/p&gt;

&lt;h1 id=&quot;creating-plots&quot;&gt;Creating Plots&lt;/h1&gt;

&lt;p&gt;We will now go through the more common plots used in data science, although as has been stated before, there are many more plots available in the Seaborn library. All Seaborn plots follow a similar structure, although there are some unique attributes depending on the plot type. Thus, the basic format for Seaborn plots is as follows &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sns.plottype(data = dataset, x = variable, y = response, ...)&lt;/code&gt;. I will also post the code I used for the plots I created so that the format is made clearer and explain the new attributes associated with each plot.&lt;/p&gt;

&lt;p&gt;As I go through each plot covered in this post, I encourage the reader to view each plot’s reference page for more information regarding each plot.&lt;/p&gt;

&lt;h3 id=&quot;scatterplot&quot;&gt;Scatterplot&lt;/h3&gt;

&lt;p&gt;To create a scatterplot the code is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sns.scatterplot(data = diamonds, x = 'carat', y = 'price', hue = 'color', size = 'clarity', alpha = 0.5)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-26/scatterplot.png&quot; alt=&quot;scatter&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hue allows to user to make each unique variable delcared a different color, so in the plot above each color represents a unique diamond color.
Size is similar to the hue attribute as it allows the size of each dot to be different depending on the unique values of the variable provided.
Alpha just adds a level of opacity so that data points hidden behind one another are more easily seen.&lt;/p&gt;

&lt;h3 id=&quot;facetgrid&quot;&gt;FacetGrid&lt;/h3&gt;

&lt;p&gt;The Facet Grid method of the Seaborn library allows two unique values of two variables to be compared to each other. Loading this type of plot can take a while depending on the number of unique variable values provided since there will be n x m plots, with n being the number of levels in variable 1 and m the number of levels in variable 2. The code I used to create the FacetGrid is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fg = sns.FacetGrid(diamonds, col = 'color', row = 'clarity')
fg.map(sns.scatterplot, 'carat', 'price')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-26/grid.png&quot; alt=&quot;grid&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using this method the type of plot must be declared in the map function. As may be gathered from the code above the plot compares each level of color with each level of clarity, so there are a lot of plots. This kind of plot is best used for a low number of levels.&lt;/p&gt;

&lt;h3 id=&quot;box-plot&quot;&gt;Box Plot&lt;/h3&gt;

&lt;p&gt;The code for a box plot is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;box = sns.boxplot(data = diamonds, x = 'color', y = 'price', hue = 'cut')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-26/boxplot.png&quot; alt=&quot;boxplot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hue does the exact same thing as hue in the scatterplot.&lt;/p&gt;

&lt;h3 id=&quot;histogram&quot;&gt;Histogram&lt;/h3&gt;

&lt;p&gt;The code for a histogram is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sns.histplot(diamonds, x=&quot;price&quot;, hue=&quot;cut&quot;, multiple=&quot;stack&quot;, edgecolor=&quot;.3&quot;, linewidth=.5, log_scale=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-26/hist.png&quot; alt=&quot;hist&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Multiple allows the user to decide what to do with multiple levels. By looking at the sns.histplot reference page, one can see the options available.
Edgecolor changes the opacity of the bar outlines.
Linewidth affects the outline width for each bar.
log_scale affects the axes values so that they’re either set to the log scale or not.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Of course there are other attributes involved with the Seaborn plots such as changing the color palette of the plots, which can be explored at the reader’s leisure. Changing plot size, axis values, plot titles, etc., may be adjusted through matplotlib. Hopefully, this blog post has piqued interest in exploring more of Seaborn’s features and anything the reader is unsure of may be Googled or reference using the Seaborn reference page. To learn more about the Seaborn package, one may visit https://seaborn.pydata.org/index.html.&lt;/p&gt;</content><author><name>Alexander Norris</name><email>alexandernorris333@gmail.com</email></author><category term="Seaborn" /><category term="Data Visualization" /><category term="Statistics Plots" /><summary type="html">Introduction The Seaborn library is a Python package which creates data visualizations based on the matplotlib library. As a result, this blog post will assume that the reader has a basic understanding of the matplotlib library. The main draw to the Seaborn library is its ability to create more visually appealing data plots than the matplotlib. Additionally, Seaborn plots contain more attributes which allow plots to contain more characteristics than their matplotlib counterparts. Throughout this blog post, we will go over a few different plots and the characteristics available through the Seaborn library. Of course this post will by no means be exhaustive. Thus, the reader is encouraged to delve further on their own for more information regarding the remarkable abilities of Seaborn plots. For those interested more examples can be found at https://seaborn.pydata.org/examples/index.html which displays a variety of various plots some of which the reader may be unaware exists. Setting Up For this post I will be using the diamonds dataset which comes with the Seaborn library. There are a number of datasets which come with Seaborn which can be used for exploring Seaborn plots, one can Google the Seaborn datasets if the reader desires to explore the datasets. If one desires to use a Seaborn dataset they can load the data using the following code: sns.load_dataset(&quot;dataset&quot;), otherwise load in the data using other methods the reader is familiar with. To the observant reader, one can deduce that the Python shortcut for Seaborn is sns, thus, loading the Seaborn library would be as follows: import seaborn as sns import matplotlib.pyplot as plt It is usual to also include the matplotlib plot library when using seaborn since both libraries can be used together. In fact matplotlib is used for plot titles, legends, and other custom alterations to the plot. Creating Plots We will now go through the more common plots used in data science, although as has been stated before, there are many more plots available in the Seaborn library. All Seaborn plots follow a similar structure, although there are some unique attributes depending on the plot type. Thus, the basic format for Seaborn plots is as follows sns.plottype(data = dataset, x = variable, y = response, ...). I will also post the code I used for the plots I created so that the format is made clearer and explain the new attributes associated with each plot. As I go through each plot covered in this post, I encourage the reader to view each plot’s reference page for more information regarding each plot. Scatterplot To create a scatterplot the code is as follows: sns.scatterplot(data = diamonds, x = 'carat', y = 'price', hue = 'color', size = 'clarity', alpha = 0.5) Hue allows to user to make each unique variable delcared a different color, so in the plot above each color represents a unique diamond color. Size is similar to the hue attribute as it allows the size of each dot to be different depending on the unique values of the variable provided. Alpha just adds a level of opacity so that data points hidden behind one another are more easily seen. FacetGrid The Facet Grid method of the Seaborn library allows two unique values of two variables to be compared to each other. Loading this type of plot can take a while depending on the number of unique variable values provided since there will be n x m plots, with n being the number of levels in variable 1 and m the number of levels in variable 2. The code I used to create the FacetGrid is as follows: fg = sns.FacetGrid(diamonds, col = 'color', row = 'clarity') fg.map(sns.scatterplot, 'carat', 'price') Using this method the type of plot must be declared in the map function. As may be gathered from the code above the plot compares each level of color with each level of clarity, so there are a lot of plots. This kind of plot is best used for a low number of levels. Box Plot The code for a box plot is as follows: box = sns.boxplot(data = diamonds, x = 'color', y = 'price', hue = 'cut') Hue does the exact same thing as hue in the scatterplot. Histogram The code for a histogram is as follows: sns.histplot(diamonds, x=&quot;price&quot;, hue=&quot;cut&quot;, multiple=&quot;stack&quot;, edgecolor=&quot;.3&quot;, linewidth=.5, log_scale=True) Multiple allows the user to decide what to do with multiple levels. By looking at the sns.histplot reference page, one can see the options available. Edgecolor changes the opacity of the bar outlines. Linewidth affects the outline width for each bar. log_scale affects the axes values so that they’re either set to the log scale or not. Conclusion Of course there are other attributes involved with the Seaborn plots such as changing the color palette of the plots, which can be explored at the reader’s leisure. Changing plot size, axis values, plot titles, etc., may be adjusted through matplotlib. Hopefully, this blog post has piqued interest in exploring more of Seaborn’s features and anything the reader is unsure of may be Googled or reference using the Seaborn reference page. To learn more about the Seaborn package, one may visit https://seaborn.pydata.org/index.html.</summary></entry><entry><title type="html">Check your Assumptions</title><link href="/blog/Linear-Assumptions" rel="alternate" type="text/html" title="Check your Assumptions" /><published>2021-10-25T00:00:00-07:00</published><updated>2021-10-25T00:00:00-07:00</updated><id>/blog/Linear-Assumptions</id><content type="html" xml:base="/blog/Linear-Assumptions">&lt;h3 id=&quot;if-you-torture-the-data-enough-nature-will-always-confess--ronald-coase&quot;&gt;“If you torture the data enough, nature will always confess.” – Ronald Coase&lt;/h3&gt;

&lt;h2 id=&quot;the-set-up&quot;&gt;The Set Up&lt;/h2&gt;

&lt;p&gt;Throughout this blog post, we will be going back to the basics: statistical assumptions.&lt;/p&gt;

&lt;p&gt;Of course, we all know that linear regressions are the most basic but also one of the most used statistical modeling techniques. However, on rare occasions we consider whether the linear regression’s assumptions are being met on the underlying distribution of the data for our models to fully succeed. In specific, we need to check whether certain assumptions are being met in order to draw inferences from the model.&lt;/p&gt;

&lt;p&gt;Today we will go over 6 assumptions for a Linear Regression:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assumption #1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Linearity: the relationship between the independent(s) variable(s) and the dependent variable is/are linear! That one is pretty intuitive, right?!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assumption #2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The residuals are normally distributed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assumption #3&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Homoscedasticity: the variance of the residuals is constant across all values of the independent(s) variable(s).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assumption #4&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;No multicollinearity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assumption #5&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;No missing variables.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assumption #6&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Independence: the observations are not correlated with each other.&lt;/p&gt;

&lt;p&gt;Before diving into it, let’s start with the data.&lt;/p&gt;

&lt;p&gt;This data set can be found at https://archive.ics.uci.edu/ml/datasets/Wine+Quality and it collects data about red wine.&lt;/p&gt;

&lt;p&gt;Here is what this data set looks like:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'wine.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-25/FIGURE1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, before we start with each one of the assumptions, we need to start with fitting the linear regression model. There are many ways to do that, but here we will use sklearn and statsmodel. Either way is fine, of course, so feel free to pick whatever is more convinient.&lt;/p&gt;

&lt;p&gt;From this data set, we will try to predict alcohol levels, the dependent variable, using citric acid, residual sugar, and quality as independent variables. For the sake of learning, we will use these variables because we have reasons to expect some of the linear regression assumptions to be violated in this case.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Set up variables
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'residual sugar'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'citric acid'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'quality'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'alcohol'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'residual sugar'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'citric acid'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'quality'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'alcohol'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Instantiate model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lin_mod&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Fit model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lin_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Print intercept and coefficient
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Intercept: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lin_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intercept_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Coefficients: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lin_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;statsmodels.api&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;## Refit the model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lin_mod&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OLS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lin_mod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-25/FIGURE2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see a summary of the model, including the coefficients and the intercept in the ouput above.&lt;/p&gt;

&lt;p&gt;Now, to the assumptions!&lt;/p&gt;

&lt;h2 id=&quot;assumption-1&quot;&gt;Assumption #1&lt;/h2&gt;

&lt;p&gt;Linearity: the relationship between the independent(s) variable(s) and the dependent variable is/are linear.&lt;/p&gt;

&lt;p&gt;The most beautiful way to check for this assumption is using Seaborn, through scatter plots.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pairplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-25/FIGURE4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To find out whether there this assumption is met, we need to check the scatter plots between the dependent variable “alcohol” and each of the independent variables seem to be linear.&lt;/p&gt;

&lt;p&gt;Unfortunately, there does not seem to be a very strong linear relationship (or any discernible relationship really) between any of the independent variables and the dependent variable. It is also noticeable that quality is a categorical variable.&lt;/p&gt;

&lt;h2 id=&quot;assumption-2&quot;&gt;Assumption #2&lt;/h2&gt;

&lt;p&gt;The residuals are normally distributed.&lt;/p&gt;

&lt;p&gt;We can test this assumption by doing a histogram or a QQplot. Let’s focus on how to make the QQplot.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;## Get the residuals
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;residuals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resid&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plot!
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qqplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;residuals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'45'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-25/FIGURE5.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the QQplot, we can check if all the points are on the red straight line. In the graph above, we can see that there is significant curvature to the line and some disperse points at the right and left. Therefore, we can say that the data does not satisfactorily meet this assumption.&lt;/p&gt;

&lt;h2 id=&quot;assumption-3&quot;&gt;Assumption #3&lt;/h2&gt;

&lt;p&gt;Homoscedasticity: the variance of the residuals is constant across all values of the independent(s) variable(s).&lt;/p&gt;

&lt;p&gt;One way to check for homoscedasticity is to make a scatterplot with the residuals and the dependent variable.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;residuals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-25/FIGURE6.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Wow. Okay… I guess not.
Homoscedasticity means the error or the residuals are constant, so we should see a cloud-like figure on the scatterplot. There aren’t just lines (because of the categorical variables) in the present scatterplot, but there is also a clear positive relationship, which is no good when checking this assumption.
We have a clear case of heteroscedasticity, and this assumption is not violated.&lt;/p&gt;

&lt;h2 id=&quot;assumption-4&quot;&gt;Assumption #4&lt;/h2&gt;

&lt;p&gt;No multicollinearity. This just means that the independent variables are not correlated between themselves. In the case that they are correlated, this indicates that more than one variable explains the same information from the raw data, which can cause problems in modeling the data.&lt;/p&gt;

&lt;p&gt;One way to check for multicollinearity is using the Variance Inflation Factor, or just VIF.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;statsmodels.stats.outliers_influence&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variance_inflation_factor&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variance_inflation_factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-25/FIGURE7.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Okay, but how to interpret the mysterious numbers above? First, recall that VIFs start from 1 and go to infinity (well, there is no upper limit). The lower the VIF, the better, since it means there is lower multicollinearity or correlation between the variables. Generally speaking, a VIF greater than 6 can cause trouble.&lt;/p&gt;

&lt;p&gt;In our selected independent variables, however, the VIFs are pretty close to 1, so there are no problems with multicollinearity here.&lt;/p&gt;

&lt;h2 id=&quot;assumption-5&quot;&gt;Assumption #5&lt;/h2&gt;

&lt;p&gt;Assumption 5 is that the model uses all important independent variables that explain the dependent variable. In this case, we are trying to predict alcohol levels using information about citric acid, residual sugar, and quality, which are intuitively and logically not enough to predict alcohol levels. In reality, we might have good reasons do doubt there is any correlation between some of these variables and the response in the first place.&lt;/p&gt;

&lt;p&gt;In this dataset, we have other variables that could be incorporated to the model in order to predict alcohol levels, including perhaps total sulfur dioxide or even sulphates. Of course, we can also think of many other explanatory variables to add to this model, including brand or age of the wine, for example.&lt;/p&gt;

&lt;h2 id=&quot;assumption-6&quot;&gt;Assumption #6&lt;/h2&gt;

&lt;p&gt;Our last assumption is independence, or all observations are not correlated with each other. We can say that this assumption has little to do with our modeling or anything observable in the data, but it’s mostly about data collection.&lt;/p&gt;

&lt;p&gt;The most classic example of independent variables are randomly assigned or collected observations, like in experimental studies where each different treatment can be randomly assigned.&lt;/p&gt;

&lt;p&gt;In the present case, we have no information on how the data was collected, and therefore, we cannot infer that this assumption was satisfied.&lt;/p&gt;

&lt;h2 id=&quot;comment-below&quot;&gt;Comment below!&lt;/h2&gt;

&lt;p&gt;And that’s it! We just went through the process of verifying assumptions for linear regressions. Please comment below on other ways we can check these assumptions or ways we can deal with the data when you notice of the assumptions are violated. And remember, it is important to verify assumptions every time we are dealing with parametric models, like the linear regression, in order to draw inferences and fit a good model.&lt;/p&gt;</content><author><name>Rachel Pourre</name><email>rachelpourre@gmail.com</email></author><category term="Assumptions" /><category term="Linear Regression" /><category term="Python" /><summary type="html">“If you torture the data enough, nature will always confess.” – Ronald Coase</summary></entry><entry><title type="html">Making Data Science Stick</title><link href="/blog/Learning-Data-Science" rel="alternate" type="text/html" title="Making Data Science Stick" /><published>2021-10-23T00:00:00-07:00</published><updated>2021-10-23T00:00:00-07:00</updated><id>/blog/Learning-Data-Science</id><content type="html" xml:base="/blog/Learning-Data-Science">&lt;p&gt;Learning data science is not always simple. Data science is composed by knowledge from different disciplines including math, statistics, and computer science. Because learning data science requires acquiring a great amount of knowledge, it would be useful to also learn a little bit about how we learn. If we learn how to more effectively acquire new knowledge, store it, and retrieve it when necessary, we will be able to keep up with advancing technologies and become more efficient data scientists.&lt;/p&gt;

&lt;h2 id=&quot;principles-of-successful-learning&quot;&gt;Principles of Successful Learning&lt;/h2&gt;

&lt;p&gt;In this article, I will talk about seven principles mentioned in the book &lt;strong&gt;Make it Stick: The Science of Successful Learning&lt;/strong&gt;. I will then provide examples of how to apply each of these principles when learning data science.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://images-na.ssl-images-amazon.com/images/I/51lagMtiKaL.jpg&quot; alt=&quot;Make it Stick Cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The seven principles are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Retrieval practice&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Spaced, varied practice&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Desirable difficulties&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Elaboration, generation, and reflection&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The illusion of knowing&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learning preferences vs. learning styles&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Growth mindset&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;principle-1-retrieval-practice&quot;&gt;Principle 1: Retrieval practice&lt;/h2&gt;

&lt;p&gt;There is a big difference between repeating a concept to yourself after you just read it and trying to actively retrieve it. For example, when studying for a test, we might read again and again that the Central Limit Theorem is the name of the theorem stating that the sampling distribution of a statistic is approximately normal whenever the sample is large and random. After reading it several times, we might think that we know what the Central Limit Theory is. However, we would be confusing familiarity with actual knowledge. It is not until we can actively retrieve its definition from memory (without looking at the text again) that we can say that we know what the Central Limit Theorem actually is.&lt;/p&gt;

&lt;p&gt;The good news is that retrieval not only allows us to know whether we have learned something or not. It also helps us learn. Like Aristotle said, ‘the things we have to learn before we can do them, we learn by doing them.’ Trying to actively retrieve knowledge solidifies that knowledge in our memories. Therefore, we can use retrieval practice to test our knowledge, but we can also use it to help us retain it.&lt;/p&gt;

&lt;p&gt;A possible application of this principle would be to think of possible test questions while we are reading the material by the professor or while listening to lectures. This principle is particularly useful in very conceptual classes, such as statistics. Then, you can try to answer those questions.&lt;/p&gt;

&lt;p&gt;Retrieval practice will allow us to store knowledge in our long-term memory, and this is when the next principle can come in handy.&lt;/p&gt;

&lt;h2 id=&quot;principle-2-spaced-varied-practice&quot;&gt;Principle 2: Spaced, varied practice&lt;/h2&gt;

&lt;p&gt;In order to make sure that we can use information stored in our long-term memory when we need it the most, we have to practice retrieving that information at different times and in different contexts. If we try to retrieve a memory right before we are about to forget it, we are strengthening that connection in our mind. Therefore, we are consolidating that memory and engaging our long-term memory.&lt;/p&gt;

&lt;p&gt;In addition, we should switch topics during our study sessions to further consolidate our knowledge. But there’s another advantage to that, which is that we can relate what we are learning in different classes. For example, we might be taking linear algebra and linear regression. If we are studying the properties of matrices and how to perform different operations with them, and then switch to the topics to the importance of independence in linear regression, we can see how these topics are connected. We would be creating even more connections in our brains, consolidating memories and learning how to apply our knowledge in the appropriate context.&lt;/p&gt;

&lt;h2 id=&quot;principle-3-desirable-difficulties&quot;&gt;Principle 3: Desirable difficulties&lt;/h2&gt;

&lt;p&gt;While being a college student can be hard enough, we can benefit from adding - or simply embracing - certain difficulties into our learning. There are many easy ways to ‘study,’ but this doesn’t mean that they are the most efficient practices. For example, like we talked about before, it is easier to read a passage several times and think that we have learned that content just because we are familiar with the text than quizzing ourselves on those concepts.&lt;/p&gt;

&lt;p&gt;It might be counter-intuitive to think of difficulties as desirable, but when we struggle with a problem and try to come up with solutions (even if we fail), our learning improves. Results will appear to come slower, but they will be better and will last longer.&lt;/p&gt;

&lt;p&gt;Classes such as Probability and Inference or programming classes are great opportunities to try this principle. For example, we can first look at the practice problems at the end of the chapter and try to solve them, then we can read the chapter and try to solve them again. This will give you an opportunity to struggle and try to produce your own solutions. You will then be more prepared to learn the correct way to solve them.&lt;/p&gt;

&lt;h2 id=&quot;principle-4-elaboration-generation-and-reflection&quot;&gt;Principle 4: Elaboration, generation, and reflection&lt;/h2&gt;

&lt;p&gt;This principle might be the easiest to apply. If you form a study group, you can use that opportunity to explain difficult concepts to each other. Coming up with examples or analogies is what this principle is about. If you teach a difficult concept to someone else, you will be forced to elaborate and reflect on the subject. This will consolidate your own understanding and will clearly show you if you have any deficiencies in your knowledge.&lt;/p&gt;

&lt;p&gt;To summarize this principle, I like to say that to be a good student you should try to be a good teacher.&lt;/p&gt;

&lt;h2 id=&quot;principle-5-the-illusion-of-knowing&quot;&gt;Principle 5: The illusion of knowing&lt;/h2&gt;

&lt;p&gt;As statisticians, we know that our assumptions are not always met, so we need a way to test them. One assumption students commonly hold is that they understand a topic when they truly don’t. This is why we need different objective ways to test our knowledge and make sure that this assumption is met. It’s just like the common phrase: if you can’t measure it, you can’t improve it. One way to do this, which is related to previous principles, would be to test ourselves. Quizzes give us an objective measure of our knowledge since we can see how many questions we get right or wrong. In addition, we can make a note of the topic that each question is testing so that we can study that topic further.&lt;/p&gt;

&lt;p&gt;Use practice tests provided by your professors, practice questions in your textbook, and programming challenges found on the internet.&lt;/p&gt;

&lt;h2 id=&quot;principle-6-learning-preferences-vs-learning-styles&quot;&gt;Principle 6: Learning preferences vs. learning styles&lt;/h2&gt;

&lt;p&gt;This principle is very interconnected with the previous principle. Most of the time, we enjoy doing the things we are naturally good at. It’s human nature. However, what is easy is not always the same as what is best. Like we talked about before, you don’t want to focus on only doing things you are naturally good at because 1) you will think that you know more than you do and 2) it is not necessarily what promotes learning.&lt;/p&gt;

&lt;p&gt;For example, if you are good at making diagrams but bad at taking quizzes, you have to make sure that you practice both. Or, if you are good at coding but don’t really understand the underlying assumptions of the analysis you are performing, you shouldn’t focus on what you prefer or are good at, but you should instead focus on what you are lacking or need to improve so that you promote your learning and not your ego.&lt;/p&gt;

&lt;h2 id=&quot;principle-7-growth-mindset&quot;&gt;Principle 7: Growth mindset&lt;/h2&gt;

&lt;p&gt;Finally, you might feel that you are not smart enough to be a data scientist or that you are having too many difficulties. If this is the case, it is important to remember that intelligence is not set but is malleable. In addition, you will now have the right tools and learning strategies to exponentially improve your learning process. Learning is more a marathon than a sprint, and having a growth mindset will help you stay motivated to reach your goals.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://edsurge.imgix.net/uploads/post/image/12467/mind_as_muscle-1565189295.jpg?auto=compress%2Cformat&amp;amp;w=1024&amp;amp;h=512&amp;amp;fit=crop&quot; alt=&quot;Growth Mindset Conclusion&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The most important part of becoming a data scientist is having the right approach. It is not only about intelligence or hard work, it is also about putting in the right kind of work and being consistent. In addition, it is important to remember that there are many resources available to students to make their learning path easier to navigate. Learning is not something we do once and then forget about it, it is something we are doing all the time. Therefore, we might as well get good at it.&lt;/p&gt;</content><author><name>Gerardo Meza Mera</name><email>meza.mera.gerardo@gmail.com</email></author><category term="Learning" /><category term="Retrieval practice" /><category term="Study strategies" /><summary type="html">Learning data science is not always simple. Data science is composed by knowledge from different disciplines including math, statistics, and computer science. Because learning data science requires acquiring a great amount of knowledge, it would be useful to also learn a little bit about how we learn. If we learn how to more effectively acquire new knowledge, store it, and retrieve it when necessary, we will be able to keep up with advancing technologies and become more efficient data scientists.</summary></entry><entry><title type="html">Automating a Python Script on a Mac</title><link href="/blog/python-auto" rel="alternate" type="text/html" title="Automating a Python Script on a Mac" /><published>2021-10-22T00:00:00-07:00</published><updated>2021-10-22T00:00:00-07:00</updated><id>/blog/python-auto</id><content type="html" xml:base="/blog/python-auto">&lt;p&gt;Automating a python script to run on a certain interval can be extremely convenient and useful. When working with dynaimc data, automating a script can be especially convenient in order to keep up with a constant influx of new data. It can save lots of time as well, so that you don’t have to manually run your script each time you want it to be ran. No matter what your python script does, you can automate it - and that’s pretty awesome. There are multiple ways to automate a python script to run on a specified schedule, and some may be better than others. I do not claim to be an expert about any of these ways, but I can show you one way to automate a python script. This post is for Mac users, unfortunately I will not be covering the different nuances for Windows users. I will essentially be covering the three steps necessary to automating/scheduling a python script. These steps are not limited to python files, but that will be the example I am using. The following steps can be summed up as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Write a python script.&lt;/li&gt;
  &lt;li&gt;Make the script executable.&lt;/li&gt;
  &lt;li&gt;Automate/schedule the script to run.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;lets-get-started&quot;&gt;Let’s get started!&lt;/h1&gt;

&lt;h2 id=&quot;1&quot;&gt;1&lt;/h2&gt;
&lt;p&gt;First up, you need a python script that you want to run. The example I am using is a python script that is pulling COVID data from an API, converting the data into a pandas dataframe, and saving the data to a csv. Obviously there is much more you can do than just save a csv. Your script can send an email, populate a database, update a dashboard, etc. This is just a simple example, the focus of this post is meant to be on the automation of the script and not on the script itself. My code is fairly simple, but don’t worry too much about all the specifics of the code, I will comment next to the relevant lines.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blogimages/figs-10-22/pyscript.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see from the comments in my code, there are a few important things to notice. First off is setting the path where you want the file to be downloaded. If you forget this step, it might take you a second to find where the output of your file was downloaded. In order to stay organized, I have set the file path so that I know where to find the output from my script. Additionally, getting the timestamp is really helpful. Because I add the timestamp to the name of my file, I will be able to discern when a specific file was output from my python script. Lastly, saving my data to a csv is essentially my end deliverable. I could have saved it in a different format,or perhaps emailed the data somewhere or uploaded it elsewhere, there are several possibilites as I mentioned earlier. In this case, I wanted to donwload a file each time the script ran so I could be extra sure that the automation of the file was working correctly.&lt;/p&gt;

&lt;h2 id=&quot;2&quot;&gt;2&lt;/h2&gt;
&lt;p&gt;Now that I have my python script, it’s time to automate it. To do this, I first need to make my script executable. Follow the steps below to do this.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Open the terminal and run pip install pyinstaller. This will install PyInstaller.&lt;/li&gt;
  &lt;li&gt;Inside the terminal, navigate to the directory where your script is located.&lt;/li&gt;
  &lt;li&gt;Once you‘re in the path where your script is located, run the following ‘pyinstaller –onefile example.py’ in the terminal to make the script executable.&lt;/li&gt;
  &lt;li&gt;You should see a message that includes “completed successfully.” Then in the directory where your script is located, you should see a folder named “dist”. Click inside that folder and you will find the executable. You’ll need to know the path to this executable, so keep this in mind for later.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;3&quot;&gt;3&lt;/h2&gt;
&lt;p&gt;Okay now your python script is executable - hooray! Next up, we are going to automate this script to run at a specified interval by using crontab.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Open the terminal and run crontab -e.&lt;/li&gt;
  &lt;li&gt;Press ‘i’ to activate the INSERT mode.&lt;/li&gt;
  &lt;li&gt;Use https://crontab.guru/ to write how often you want the script to run.
(example: ‘* * * * *’ would make the script run every minute)&lt;/li&gt;
  &lt;li&gt;After you specify when the script will run, press SPACE, put the path to the executable you just created, press SPACE again, and put the path to your script.
(example: * * * * * /path/to/my/executable /path/to/my/script)&lt;/li&gt;
  &lt;li&gt;Press esc. and then type :wq to save and exit (w - write, q - quit) and then press enter.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To make sure that the crontab was created, type ‘crontab -l’ in the terminal and you should see it. To edit the crontab, type ‘crontab -e’. To remove the crontab, type ‘crontab -r’.&lt;/p&gt;

&lt;h2 id=&quot;congrats-you-just-automated-your-python-script&quot;&gt;Congrats, you just automated your python script!&lt;/h2&gt;
&lt;p&gt;As I mentioned, this is not the only way to automate a script, but it is one simple and effective way that I have found. I hope this post was helpful in some way and that you learned something new!&lt;/p&gt;

&lt;h3 id=&quot;a-few-things-to-know&quot;&gt;A few things to know:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;If you have don’t have the right permissions, you need to give full disk access. Click System Preferences, Security and Privacy, Privacy, and Full Disk Access and then grant access.&lt;/li&gt;
  &lt;li&gt;If for whatever reason your crontab is not working, you can type ‘mail’ in the terminal and then press enter to see potential error messages.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Rex Moss</name><email>rexxmoss@gmail.com</email></author><category term="crontab" /><category term="pandas" /><category term="terminal" /><category term="mac" /><summary type="html">Automating a python script to run on a certain interval can be extremely convenient and useful. When working with dynaimc data, automating a script can be especially convenient in order to keep up with a constant influx of new data. It can save lots of time as well, so that you don’t have to manually run your script each time you want it to be ran. No matter what your python script does, you can automate it - and that’s pretty awesome. There are multiple ways to automate a python script to run on a specified schedule, and some may be better than others. I do not claim to be an expert about any of these ways, but I can show you one way to automate a python script. This post is for Mac users, unfortunately I will not be covering the different nuances for Windows users. I will essentially be covering the three steps necessary to automating/scheduling a python script. These steps are not limited to python files, but that will be the example I am using. The following steps can be summed up as:</summary></entry></feed>